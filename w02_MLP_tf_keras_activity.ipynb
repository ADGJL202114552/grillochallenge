{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w02_MLP_tf_keras_activity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Building multilayer perceptron (MLP) in tf.keras\n",
        "\n",
        "This code is structured to cover **binary and multi-class classification**, as well as **regresion** using different MLP implementations.\n",
        "\n",
        "Main references:\n",
        "\n",
        "* [TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras](https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/)\n",
        "* [How to create an MLP classifier with TensorFlow 2 and Keras](https://www.machinecurve.com/index.php/2019/07/27/how-to-create-a-basic-mlp-classifier-with-the-keras-sequential-api/)\n",
        "\n",
        "---\n",
        "\n",
        "## The 5-Step Model Life-Cycle\n",
        "\n",
        "The knowledge about the model life-cycle provides the backbone for both modeling a dataset and understanding the `tf.keras` API.\n",
        "\n",
        "The five steps in the life-cycle are as follows:\n",
        "\n",
        "* Define the model\n",
        "  * `model = ...`\n",
        "* Compile the model\n",
        "  * `optimizer = ...`\n",
        "  * `loss_function = ...`\n",
        "  * `metrics = ...`\n",
        "  * `model.compile (optimizer, loss_function, metrics)`\n",
        "* Fit the model\n",
        "  * `model.fit(X_training_features, y_training_labels, epochs, batch_size, verbose=2)`\n",
        "* Evaluate the model\n",
        "  * `loss = model.evaluate(X_testing_features, y_testing_labels, verbose=0)`\n",
        "* Make predictions\n",
        "  * `y_predicted = model.predict(X_new_data)`\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Defining the model\n",
        "\n",
        "### 1.1) the functional API\n",
        "\n",
        "The [Keras functional API](https://www.tensorflow.org/guide/keras/functional) is a way to create models that are more flexible than the `tf.keras.Sequential` API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
        "\n",
        "The main idea is that a deep learning model is usually a *directed acyclic graph (DAG)* of layers. So the functional API is a way to build **graphs of layers**.\n",
        "\n",
        "In order to build a model using the functional API, we need to explicity connect the output from one layer to the input to the next layer.\n",
        "\n",
        "Suppose we want to define a model that receives 8 features as input, has a hidden layer with 10 nodes and generates a prediction for a numerical value as output."
      ],
      "metadata": {
        "id": "UvTklNQr4Xr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a model defined with the functional API\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# the input layer is defined through the Input class\n",
        "x_in = Input(shape=(8,))\n",
        "# Next, a fully connected layer can be connected to the input by calling \n",
        "# the layer and passing the input layer. This will return a reference to the \n",
        "# output connection in this new layer.\n",
        "x_hidden = Dense(10)(x_in)\n",
        "# Finally, we connect the output layer\n",
        "x_out = Dense(1)(x_hidden)\n",
        "\n",
        "# then, we define the model by specifying the input and output layers\n",
        "model2 = Model(inputs=x_in, outputs=x_out)"
      ],
      "metadata": {
        "id": "jpw1D3KC92Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2) the `Sequential` API\n",
        "\n",
        "We will be designing some MLP models using the `Sequential` API.\n",
        "\n",
        "---\n",
        "\n",
        "## CASE STUDY 1: MLP for binary classification\n",
        "\n",
        "We will use the **Ionosphere** dataset to demonstrate an MLP for binary classification.\n",
        "\n",
        "This dataset involves predicting whether a structure is in the atmosphere or not given radar returns.\n",
        "\n",
        "The dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n",
        "\n",
        "* [Ionosphere Dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv) (csv).\n",
        "* [Ionosphere Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.names)\n"
      ],
      "metadata": {
        "id": "HuadTJGwAL0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "MTiOkJTa92a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Data ingestion/acquisition"
      ],
      "metadata": {
        "id": "IqDTpJYwL-XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "df = read_csv(path, header=None)"
      ],
      "metadata": {
        "id": "KRGGkhxptmSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "SJ-n52aiuNA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Data pre-processing\n",
        "\n",
        "We need to separate input features (`X`) and labels/targets (`y`).\n",
        "\n",
        "We also use a `LabelEncoder` to encode the class labels (strings) as integer values 0 and 1. \n",
        "\n",
        "The model will be fit on 67 percent of the data (`X_train`, `y_train`), and the remaining 33 percent will be used for evaluation (`X_test`, `y_test`). We use the `train_test_split()` function for that."
      ],
      "metadata": {
        "id": "rIY_WUgDLuMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into input features (X) and class labels/targets (y)\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# ensure all data are floating point values\n",
        "X = X.astype('float32')\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "# split into training (67%) and testing (33%) samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "6l6uC2SQtmVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]\n",
        "n_features"
      ],
      "metadata": {
        "id": "l-A1DJOvvKIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Model definition using the `Sequential` API\n",
        "\n",
        "* **How many layers should we use?**\n",
        "* **How many nodes in each layer?**"
      ],
      "metadata": {
        "id": "EZfIaDetMRSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model predicts the probability of class 1 and uses the `sigmoid` activation function. "
      ],
      "metadata": {
        "id": "mifh_R2FcdlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "fnN6YiQitmYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For compiling the model, we need to specify:\n",
        "\n",
        "* an [optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
        "* a [loss function](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
        "* the [metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) for evaluating the model"
      ],
      "metadata": {
        "id": "QyeQKPYRQkHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "L6MhrPN3tmbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Training the model\n",
        "\n",
        "The model should be trained based on training input features and labels.\n",
        "\n",
        "You need to specify: i) the iteration steps (`epochs`) and ii) the number of samples (`batch_size`) to be used during training."
      ],
      "metadata": {
        "id": "89UiiF1qTE-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "2CvQhLobt8gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Evaluating the model\n",
        "\n",
        "For evaluation, we make use of testing input features and labels and usually keep track of `loss` and `accuracy` figures."
      ],
      "metadata": {
        "id": "HLtjkcyzTjiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "loss, acc = # YOUR CODE HERE\n",
        "print('Test Accuracy: %.3f' % acc)"
      ],
      "metadata": {
        "id": "4QOC2J0et8ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Using the model for classification\n",
        "\n",
        "For predicting a new class label, the model should be fed with some new (unseen) data instance."
      ],
      "metadata": {
        "id": "wz95ZWDVT4Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
        "yhat = # YOUR CODE HERE\n",
        "print('Predicted: %.3f' % yhat)"
      ],
      "metadata": {
        "id": "wpnlOLXNt8lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CASE STUDY 2: \n",
        "\n",
        "We will use the **Iris flowers** dataset to demonstrate an MLP for multiclass classification.\n",
        "\n",
        "This problem involves predicting the species of iris flower given measures of the flower.\n",
        "\n",
        "The dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n",
        "\n",
        "* [Iris Dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv) (csv).\n",
        "* [Iris Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names)."
      ],
      "metadata": {
        "id": "Yfzq6aERAd65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='/content/iris-dataset.png', width=600) "
      ],
      "metadata": {
        "id": "lqqWjaF0aeqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "TOAast9Z92eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data exploration (just for demonstration)\n",
        "\n",
        "The [Seaborn](https://seaborn.pydata.org/) library also has this dataset for demonstration purposes."
      ],
      "metadata": {
        "id": "xTHV-xgjYjZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# load dataset from Seaborn\n",
        "iris = sns.load_dataset('iris')\n",
        " \n",
        "# style used as a theme of graph\n",
        "# for example if we want black graph with grid then write \"darkgrid\"\n",
        "sns.set_style(\"whitegrid\")\n",
        " \n",
        "# sepal_length, petal_length are iris feature data \n",
        "# height is used to define the height of graph \n",
        "# hue store the class\n",
        "sns.FacetGrid(iris, hue =\"species\",\n",
        "              height = 6).map(plt.scatter,\n",
        "                              'sepal_length',\n",
        "                              'petal_length').add_legend()"
      ],
      "metadata": {
        "id": "n1Qeqd150FGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Data ingestion/acquisition\n",
        "\n",
        "We will use the Iris dataset available in the [Scikit-learn](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) library."
      ],
      "metadata": {
        "id": "f7CaN-J2Z_RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "#path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
        "#df = read_csv(path, header=None)\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "81X3Jl9lxNDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Data pre-processing\n"
      ],
      "metadata": {
        "id": "pZsTHHMDa53B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into input and output columns\n",
        "#X, y = df.values[:, :-1], df.values[:, -1]\n",
        "X, y = iris.data, iris.target\n",
        "# ensure all data are floating point values\n",
        "X = X.astype('float32')\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]"
      ],
      "metadata": {
        "id": "duVw5UocxNGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Model defiinition\n",
        "\n",
        "Given that it is a multiclass classification, the model must have one node for each class in the output layer and use the `softmax` activation function. \n",
        "\n",
        "The loss function is the `sparse_categorical_crossentropy`, which is appropriate for integer encoded class labels (e.g. 0 for one class, 1 for the next class, etc.)."
      ],
      "metadata": {
        "id": "MuRtwnImXz32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "rbZDebhZxVW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "9r7yr__-xVZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Model training"
      ],
      "metadata": {
        "id": "TNHoROW2foi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "nuOymXq8xVeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Model evaluation"
      ],
      "metadata": {
        "id": "qgA3h5Fkftnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "loss, acc = # YOUR CODE HERE\n",
        "print('Test Accuracy: %.3f' % acc)"
      ],
      "metadata": {
        "id": "wWQi24SzxVgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Classification of new data"
      ],
      "metadata": {
        "id": "aF6J_72cfyxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "row = [5.1,3.5,1.4,0.2]\n",
        "yhat = # YOUR CODE HERE\n",
        "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"
      ],
      "metadata": {
        "id": "TvEIxiujxNJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## CASE STUDY 3: MLP for Regression\n",
        "\n",
        "We will use the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) to demonstrate an MLP for regression predictive modeling.\n",
        "\n",
        "This problem involves predicting house value based on properties of the house and neighborhood.\n",
        "\n",
        "The dataset will be downloaded automatically from `Scikit-learn`.\n"
      ],
      "metadata": {
        "id": "g64N5BtNArV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "from numpy import sqrt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "cONtkBXz92hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Data ingestion/acquisition\n",
        "\n",
        "Basic characteristics of the dataset:\n",
        "\n",
        "* Number of instances: 20640\n",
        "* Number of attributes: 8 numeric, predictive attributes + the target\n",
        "* Attribute information:\n",
        "  * `MedInc`: median income in block group\n",
        "  * `HouseAge`: median house age in block group\n",
        "  * `AveRooms`: average number of rooms per household\n",
        "  * `AveBedrms`: average number of bedrooms per household\n",
        "  * `Population`: block group population\n",
        "  * `AveOccup`: average number of household members\n",
        "  * `Latitude`: block group latitude\n",
        "  * `Longitude`: block group longitude\n",
        "\n",
        "The target variable is the `median house value` for California districts, expressed in hundreds of thousands of dollars ($100,000).\n",
        "\n"
      ],
      "metadata": {
        "id": "WupzwxWmhPXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "# load the dataset\n",
        "housing = fetch_california_housing()"
      ],
      "metadata": {
        "id": "dDzTlufWhM4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.feature_names"
      ],
      "metadata": {
        "id": "Hc_sQkCo1uot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.data[0]"
      ],
      "metadata": {
        "id": "a4iqFZAo1url"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.target_names"
      ],
      "metadata": {
        "id": "U0hjhCpF1uuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.target"
      ],
      "metadata": {
        "id": "VpZV4H9u1uxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Data pre-processing"
      ],
      "metadata": {
        "id": "2sjawT_HjKmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into input and output columns\n",
        "X, y = housing.data, housing.target\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]"
      ],
      "metadata": {
        "id": "OggYS4Js1u0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Model definition\n",
        "\n",
        "This is a regression problem that involves predicting a single numerical value. As such, the output layer has a single node and uses the `default` or `linear activation function` (no activation function). The mean squared error (`mse`) loss is minimized when fitting the model."
      ],
      "metadata": {
        "id": "3BFP2yH8jpeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "cOUyCQ3JjXT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "HT6xhJGYjXW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Model training"
      ],
      "metadata": {
        "id": "t1NJhTwfj6_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "89sfXjItjXaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Model evaluation\n",
        "\n",
        "Recall that this is a regression, not classification; therefore, we cannot calculate classification accuracy. \n",
        "\n",
        "For more on this, see [this tutorial](https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/)."
      ],
      "metadata": {
        "id": "dz6qSusZj_PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "error = # YOUR CODE HERE\n",
        "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))"
      ],
      "metadata": {
        "id": "GER10UGwjXc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Making prediction from new data"
      ],
      "metadata": {
        "id": "8LnrpMwHkg7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900]\n",
        "yhat = # YOUR CODE HERE\n",
        "print('Predicted: %.3f' % yhat)"
      ],
      "metadata": {
        "id": "ITda0wkB1u3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
