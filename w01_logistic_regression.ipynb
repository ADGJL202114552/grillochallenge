{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "oPR4ivrheD8_"
      },
      "source": [
        "# Logistic regression model\n",
        "\n",
        "This notebook is organised in two parts:\n",
        "\n",
        "* the *first part* implements a **logistic regression model from scratch**, as we did for the linear regression model.\n",
        "\n",
        "* the *second part* makes use of **high-level APIs** for a concise implementation of the same logistic regression model.\n",
        "\n",
        "Part of this code is based on [this tutorial](https://towardsdatascience.com/a-logistic-regression-from-scratch-3824468b1f88)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "tensorflow"
        ],
        "id": "VmKEfc0GeD9B"
      },
      "outputs": [],
      "source": [
        "# importing necessary modules\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "\n",
        "We will be using a **modified version of the Titanic dataset** from [Kaggle](https://www.kaggle.com/azeembootwala/titanic). This version was adapted for logistic regression.\n",
        "\n",
        "There are two files namely:\n",
        "\n",
        "**`train_data.csv`**: a dataset of 792 instances and 16 features. The `survived` column is the target variable.The `parch` and `sibsp` columns from the original data set were replaced by a single column called `Family size`. \n",
        "\n",
        "All categorical data, like `Embarked` and `pclass` have been re-encoded using the one hot encoding method.Additionally, 4 more columns have been added, re-engineered from the `Name` column to `Title1` to `Title4` (Mr, Mrs, Master, Miss) signifying males and females depending on whether they were married or not. An additional analysis to see if `Married` people had more survival instincts or not, and is the trend similar for both genders.\n",
        "\n",
        "All missing values have been filled with a median of the column values. All real valued data columns have been normalized.\n",
        "\n",
        "**`testdata.csv`**: a dataset of 100 instances and 16 features, with the same arrangements made in the training dataset."
      ],
      "metadata": {
        "id": "gt8LgjadxQLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 4,
        "tab": [
          "tensorflow"
        ],
        "id": "IJr8ZZAFeD9C"
      },
      "outputs": [],
      "source": [
        "# loading the training dataset\n",
        "# REMEMBER to upload this dataset into Colab\n",
        "df1 = pd.read_csv('/content/titanic_train_data.csv')\n",
        "df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic information about the dataset (columns, data types, missing data)\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "TwwE3TuzTOju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target variable  is `Survived`; all other columns are features.\n",
        "\n",
        "A short description:\n",
        "\n",
        "* `Sex`: 0 or 1 => male or female\n",
        "* `Age`: value rescaled between 0 and 1\n",
        "* `Fare`: ticket price rescaled between 0 and 1\n",
        "* `Pclass_1` .. `Pclass_3`: One-hot encoded Passenger class\n",
        "* `Family_size`: rescaled value between 0 and 1 of family size.\n",
        "* `Title_1 .. Title_4`: mr, mrs, master, miss one-hot encoded\n",
        "* `Emb_1 .. Emb_3`: Embark location one-hot encoded.\n",
        "\n",
        "In total we will have 14 features.\n",
        "\n",
        "---\n",
        "\n",
        "### Data pre-processing\n",
        "\n",
        "Let's first remove some unnecessary columns from the dataset, as they won't be used in our model."
      ],
      "metadata": {
        "id": "46khTaezTuTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing unnecessary colums\n",
        "df1 = df1.drop(['Unnamed: 0', 'PassengerId'], axis=1)"
      ],
      "metadata": {
        "id": "PHe3RAQLTOmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example instances\n",
        "df1.sample(5)"
      ],
      "metadata": {
        "id": "we6TodnuTOoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First approach: regression model from scratch\n",
        "\n",
        "Let's split the training dataset into features (`X`) and target (`Y`) variable. We have a total of 792 examples. Therefore, the shape for `Y` is `(ùëö,1)` where `ùëö = 792`. For `X` we expect `(ùëö, 14)`, where the columns are the features."
      ],
      "metadata": {
        "id": "VgGMQCtp9nS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the training dataset into features (X) and target (Y)\n",
        "X_train = df1.iloc[:,1:].to_numpy()\n",
        "Y_train = df1['Survived'].to_numpy()"
      ],
      "metadata": {
        "id": "RXo0R1lBTOrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, Y_train.shape"
      ],
      "metadata": {
        "id": "W5Fa660vUAFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to **transpose** the input feature vector in order to perform the **dot product** necessary for the logistic regression model."
      ],
      "metadata": {
        "id": "Da08fsoC_PrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transposing the feature vector (X)\n",
        "X_train = X_train.T\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "bYId2y9yWBbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model defintion\n",
        "\n",
        "Let's start by defining the **activation function**."
      ],
      "metadata": {
        "id": "CVILEA1J_69F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom sigmoid activation function\n",
        "def sigmoid(Z):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A"
      ],
      "metadata": {
        "id": "06gUeaxcIU3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is the **forward function**, which implements the dot product and makes use of the activation function.\n",
        "\n",
        "We can split these in two steps:\n",
        "\n",
        "$$Z = WX + b$$\n",
        "$$A = \\sigma(Z)$$\n"
      ],
      "metadata": {
        "id": "dPFO6vn-Xrcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom forward pass function\n",
        "def forward(X, W, b):\n",
        "    Z = np.dot(W.T, X) + b\n",
        "    A = sigmoid(Z)\n",
        "    return A"
      ],
      "metadata": {
        "id": "r15eqnc8Xrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **loss function** should be a **binary cross entropy**, as we have only two target classes (`survided = 1` or `0`).\n",
        "\n",
        "$$loss = -\\frac{1}{m}\\sum_{i=1}^{m} y\\log(A) + (1 - y)\\log(1 - A)$$\n"
      ],
      "metadata": {
        "id": "jkClkSJ2BDai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom loss function\n",
        "# epsilon is a small value we add to avoid log(0) calculation\n",
        "def loss(A, Y, epsilon = 1e-15):\n",
        "    m = len(A)\n",
        "    l = -1/m * np.sum( Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
        "    return l"
      ],
      "metadata": {
        "id": "F25yrmWTXrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is the **backwards pass**. For this, we would need to differentiate the loss function with `W` and `b`.\n",
        "\n",
        "$$\\frac{\\partial loss}{\\partial W} \\sum_{i=1}^{m} X(A - Y)\\top$$\n",
        "\n",
        "$$\\frac{\\partial loss}{\\partial b} \\sum_{i=1}^{m} (A - y)$$\n"
      ],
      "metadata": {
        "id": "T94Kjbc0D0ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom backward pass function\n",
        "def backward(X, Y, A):\n",
        "    m = len(yhat)\n",
        "    dW = 1/m * np.dot(X, (A - Y).T)\n",
        "    db = 1/m * np.sum(A - Y) \n",
        "    return (dW, db)"
      ],
      "metadata": {
        "id": "P-xWCJYbXrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step implements the **backpropagation function** for updating weights and bias."
      ],
      "metadata": {
        "id": "Sr2K08FTI3X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom backpropagtion function for updating weights and bias\n",
        "def update(W, b, dW, db, learning_rate = 0.01):\n",
        "    W = W - learning_rate * dW\n",
        "    b = b - learning_rate * db\n",
        "    return (W, b)"
      ],
      "metadata": {
        "id": "HGtEGIRKXrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the activation function returns a probability between 0 and 1, we need a custom function to round values <= 0.5 to 0 and values > 0.5 to 1."
      ],
      "metadata": {
        "id": "eLAnLnKmIY9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom round function\n",
        "def roundValue(A):\n",
        "    return np.uint8( A > 0.5)"
      ],
      "metadata": {
        "id": "YHHpI6F_Xrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is the definition of our **accuracy metric**."
      ],
      "metadata": {
        "id": "koIdcFC3JEr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom accuracy function\n",
        "def accuracy(yhat, Y):\n",
        "    return round(np.sum(yhat==Y) / len(yhat) * 1000) / 10"
      ],
      "metadata": {
        "id": "PUSIpgRUIb_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model instantiation and training"
      ],
      "metadata": {
        "id": "GX4baICYJex_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing model parameters\n",
        "# random seed (for reproducibility)\n",
        "np.random.seed(2022)\n",
        "# we have 14 features in the dataset\n",
        "W = 0.01 * np.random.randn(14)\n",
        "# and a constant bias\n",
        "b = 0"
      ],
      "metadata": {
        "id": "CotggAnRUAOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters for training\n",
        "num_iterations = 500\n",
        "lr = 0.01\n",
        "\n",
        "# we will record loss and accuracy for plotting\n",
        "losses, acces = [], []\n",
        "# main training loop\n",
        "for i in range(num_iterations):\n",
        "    # forward pass\n",
        "    A = forward(X_train, W, b)\n",
        "    # loss calculation\n",
        "    l = loss(Y_train, A)  \n",
        "    # round the predicted value\n",
        "    yhat = roundValue(A)\n",
        "    # accuracy calculation\n",
        "    acc = accuracy(yhat, Y_train)\n",
        "    # backpropagation pass - update weights and bias\n",
        "    dW, db = backward(X_train, Y_train, A)\n",
        "    W, b = update(W, b, dW, db, learning_rate=lr)\n",
        "    # keep record of loss and accurcy\n",
        "    losses.append(l)\n",
        "    acces.append(acc)\n",
        "    # checkpoint\n",
        "    if i % 50 == 0:\n",
        "        print('loss:', l, f'\\taccuracy: {accuracy(yhat, Y_train)}%') "
      ],
      "metadata": {
        "id": "dZXMYSYxXrcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising training performance"
      ],
      "metadata": {
        "id": "DEC6mcacKXgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with plt.xkcd():\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
        "  ax.plot(np.arange(len(losses)), losses, 'b-', label='loss')\n",
        "  xlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('loss')"
      ],
      "metadata": {
        "id": "LVCylxMMWfnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with plt.xkcd():\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
        "  ax.plot(np.arange(len(acces)), acces, 'b-', label='accuracy')\n",
        "  xlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('accuracy')"
      ],
      "metadata": {
        "id": "VzBLayqRWfqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model over the testing dataset"
      ],
      "metadata": {
        "id": "zUTkIICmKbbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the testing dataset\n",
        "# REMEMBER to upload this dataset into Google Colab\n",
        "df2 = pd.read_csv('/content/titanic_test_data.csv')\n",
        "df2.shape"
      ],
      "metadata": {
        "id": "YVC6sMIZWfse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data pre-processing step\n",
        "df2 = df2.drop(['Unnamed: 0', 'PassengerId'], axis=1)\n",
        "X_test = df2.iloc[:,1:].to_numpy()\n",
        "Y_test = df2['Survived'].to_numpy()\n",
        "X_test = X_test.T\n",
        "X_test.shape"
      ],
      "metadata": {
        "id": "esOx1P_3WfuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing loop\n",
        "# NOTICE that we are keeping the weights and bias from the trained model\n",
        "num_iterations = 500\n",
        "lr = 0.01\n",
        "\n",
        "losses, acces = [], []\n",
        "# main loop\n",
        "for i in range(num_iterations):\n",
        "    A = forward(X_test, W, b)\n",
        "    l = loss(Y_test, A)  # loss function\n",
        "    yhat = roundValue(A)\n",
        "    acc = accuracy(yhat, Y_test)\n",
        "    dW, db = backward(X_test, Y_test, A)\n",
        "    W, b = update(W, b, dW, db, learning_rate=lr)\n",
        "    losses.append(l)\n",
        "    acces.append(acc)\n",
        "    if i % 50 == 0:\n",
        "        print('loss:', l, f'\\taccuracy: {accuracy(yhat, Y_test)}%') "
      ],
      "metadata": {
        "id": "znqyWdXKWfw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualising model's performance over the testing data\n",
        "with plt.xkcd():\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    ax[0].plot(np.arange(len(losses)), losses, 'b-', label='loss')\n",
        "    xlab, ylab = ax[0].set_xlabel('epoch'), ax[0].set_ylabel('loss')\n",
        "    ax[1].plot(np.arange(len(acces)), acces, 'b-', label='accuracy')\n",
        "    xlab, ylab = ax[1].set_xlabel('epoch'), ax[1].set_ylabel('accuracy')"
      ],
      "metadata": {
        "id": "ANG1l-JC19Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second approach: regression model using high-level APIs\n",
        "\n",
        "We are going to read the input features and target variable again, as we need an unmodified (i.e., not transposed) version of the data."
      ],
      "metadata": {
        "id": "ZeR6Z62y2ONR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading training features and target variable\n",
        "X_train = df1.iloc[:,1:].to_numpy()\n",
        "Y_train = df1['Survived'].to_numpy()"
      ],
      "metadata": {
        "id": "uYIzstciWf8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating the model with only one layer\n",
        "model = tf.keras.Sequential([\n",
        "    # dense layer with 14 input features, one output, and sigmoid activation function\n",
        "    tf.keras.layers.Dense(units=1, input_shape=[14], activation='sigmoid'),\n",
        "])"
      ],
      "metadata": {
        "id": "Rb4QC5glUAQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model hyperparameters: optimizer, loss function, and performance metric\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "jKfGOOcsUATT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and keeping track of loss and accuracy\n",
        "train_history = model.fit(X_train, Y_train, epochs=50)"
      ],
      "metadata": {
        "id": "gi3LzXQ9Uyxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting weights and bias from the trained model\n",
        "W_tf, b_tf = [x.numpy() for x in model.weights]\n",
        "W_tf, b_tf"
      ],
      "metadata": {
        "id": "CL46x2KVUy0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualising model's performance\n",
        "with plt.xkcd():\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
        "  ax.plot(np.arange(50), train_history.history['loss'], 'b-', label='loss')\n",
        "  xlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('loss')"
      ],
      "metadata": {
        "id": "_yWninO1Uy2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with plt.xkcd():\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
        "  ax.plot(np.arange(50), train_history.history['acc'], 'b-', label='accuracy')\n",
        "  xlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('accuracy')"
      ],
      "metadata": {
        "id": "_U6EVThfUy4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = df2.iloc[:,1:].to_numpy()\n",
        "Y_test = df2['Survived'].to_numpy()\n",
        "X_test.shape"
      ],
      "metadata": {
        "id": "GzDvRU6wSkzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the model for predicting values\n",
        "ynew = model.predict(X_test)\n",
        "# show the inputs and predicted outputs\n",
        "for i in range(len(X_test)):\n",
        "\tprint(\"Input target X: %s, Predicted target: %s\" % (X_test[i][0], ynew[i]))"
      ],
      "metadata": {
        "id": "iRrVL3InUy7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking model's performance\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "MsjHoEL7Vcfb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "w01_logistic-regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}