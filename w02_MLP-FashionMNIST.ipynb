{"cells":[{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"K_mDSl05atkj"},"source":["# Multilayer perceptrons (MLPs)\n","\n","Now that we have characterised multilayer perceptrons (MLPs) mathematically (see Lecture's material), let us try to implement one ourselves. \n","\n","This code is organised in two parts:\n","\n","* the first part brings a MLP implementation from scratch, passing through all the necessary steps.\n","* the second part makes use of `keras Sequential model` API for a concise implementation. \n"]},{"cell_type":"markdown","source":["---\n","\n","## Note for ST456\n","\n","The following command is necessary for downloading some helper functions in TensorFlow used by the reference book.\n","\n","If you get a message saying **you need to restart the runtime**, please **do so before** running the rest of the code."],"metadata":{"id":"uGOX-z9q4S3j"}},{"cell_type":"code","source":["!pip install d2l==0.17.1"],"metadata":{"id":"ek2t6O4HayyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":3,"tab":["tensorflow"],"id":"KbCAadcIatkr"},"outputs":[],"source":["# importing necessary libraries\n","import tensorflow as tf\n","from d2l import tensorflow as d2l"]},{"cell_type":"markdown","source":["### Loading the dataset\n","\n","To compare against our previous results\n","achieved with softmax regression (**see Week 01 - Homework**), we will continue to work with\n","the Fashion-MNIST image classification dataset."],"metadata":{"id":"5s5xGXk_DJpF"}},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":4,"tab":["tensorflow"],"id":"wNS4-Wdwatkt"},"outputs":[],"source":["# minibatch size\n","batch_size = 256\n","# load the dataset\n","train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"]},{"cell_type":"markdown","metadata":{"origin_pos":5,"id":"x6O-LZJ3atku"},"source":["## First approach: MLP from scratch\n","\n","### Initializing model parameters\n","\n","Recall that Fashion-MNIST contains 10 classes,\n","and that each image consists of a $28 \\times 28 = 784$\n","grid of grayscale pixel values.\n","Again, we will disregard the spatial structure\n","among the pixels for now,\n","so we can think of this as simply a classification dataset\n","with 784 input features and 10 classes.\n","\n","To begin, we will **implement an MLP\n","with one hidden layer and 256 hidden units.**\n","Note that we can regard both of these quantities\n","as hyperparameters. Typically, we choose layer widths in powers of 2,\n","which tend to be computationally efficient because\n","of how memory is allocated and addressed in hardware.\n","\n","Again, we will represent our parameters with several tensors.\n","Note that *for every layer*, we must keep track of\n","one weight matrix and one bias vector.\n","As always, we allocate memory\n","for the gradients of the loss with respect to these parameters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":8,"tab":["tensorflow"],"id":"OmVorg23atkv"},"outputs":[],"source":["# hyperparameters for the MLP\n","num_inputs, num_outputs, num_hiddens = 784, 10, 256\n","\n","W1 = tf.Variable(tf.random.normal(shape=(num_inputs, num_hiddens), mean=0, stddev=0.01))\n","b1 = tf.Variable(tf.zeros(num_hiddens))\n","W2 = tf.Variable(tf.random.normal(shape=(num_hiddens, num_outputs), mean=0, stddev=0.01))\n","b2 = tf.Variable(tf.random.normal([num_outputs], stddev=.01))\n","\n","params = [W1, b1, W2, b2]"]},{"cell_type":"markdown","metadata":{"origin_pos":9,"id":"YsgYU2H_atkw"},"source":["### Activation function\n","\n","To make sure we know how everything works, for now,\n","we will **implement the ReLU activation** ourselves\n","using the maximum function rather than\n","invoking the built-in `relu` function directly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":12,"tab":["tensorflow"],"id":"a1LR-vlFatkx"},"outputs":[],"source":["# custom implementation of the ReLU function\n","def relu(X):\n","    return tf.math.maximum(X, 0)"]},{"cell_type":"markdown","metadata":{"origin_pos":13,"id":"kp8MtrNiatky"},"source":["### Model defintion\n","\n","Because we are disregarding spatial structure,\n","we `reshape` each two-dimensional image into\n","a flat vector of length  `num_inputs`.\n","\n","Finally, we **implement our model** with just a few lines of code.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":16,"tab":["tensorflow"],"id":"19D6xE3batky"},"outputs":[],"source":["# custom MLP model\n","def net(X):\n","    # input layer\n","    X = tf.reshape(X, (-1, num_inputs))\n","    # hidden layer\n","    H = relu(tf.matmul(X, W1) + b1)\n","    # output layer\n","    return tf.matmul(H, W2) + b2"]},{"cell_type":"markdown","metadata":{"origin_pos":17,"id":"9SIZ68GPatkz"},"source":["### Loss function\n","\n","To ensure numerical stability,\n","and because we already implemented\n","loss functions from scratch in Week 01,\n","we leverage the integrated function from high-level APIs\n","for calculating the cross-entropy loss.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":20,"tab":["tensorflow"],"id":"epkLjGniatk1"},"outputs":[],"source":["def loss(y_hat, y):\n","    return tf.losses.sparse_categorical_crossentropy(y, y_hat, from_logits=True)"]},{"cell_type":"markdown","metadata":{"origin_pos":21,"id":"VtNRlzz6atk2"},"source":["### Training the model\n","\n","Fortunately, **the training loop for MLPs\n","is exactly the same as for logistic (softmax) regression**.\n","\n","Leveraging the `d2l` package again,\n","we call the `train_ch3` function,\n","setting the number of epochs to 10 and the learning rate to 0.1.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":24,"tab":["tensorflow"],"id":"eiPhvU4Patk2"},"outputs":[],"source":["# training parameters\n","num_epochs, lr = 10, 0.1\n","# default updater is SGD in D2L\n","updater = d2l.Updater([W1, W2, b1, b2], lr)\n","# training the model\n","d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"]},{"cell_type":"markdown","metadata":{"origin_pos":25,"id":"tuHGHapOatk4"},"source":["### Testing the model\n","\n","To evaluate the learned model,\n","we **apply it on some test data**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":26,"tab":["tensorflow"],"id":"o4PySK8iatk4"},"outputs":[],"source":["d2l.predict_ch3(net, test_iter)"]},{"cell_type":"markdown","metadata":{"origin_pos":27,"id":"0N8m0mvWatk4"},"source":["### Summary of the first approach\n","\n","* Implementing a simple MLP is relatively easy, even when done manually.\n","* However, with a large number of layers, implementing MLPs from scratch can get messy (e.g., naming and keeping track of our model's parameters).\n"]},{"cell_type":"markdown","source":["## Second approach: using `keras` Sequential model\n","\n","### Model definition\n","\n","As compared with our concise implementation\n","of softmax regression implementation (Week 01),\n","the only difference is that we add\n","*two fully-connected layers*: the first is **our hidden layer**,\n","which contains 256 hidden units\n","and applies the ReLU activation function, and the second is our output layer."],"metadata":{"id":"SqCxgQdCGKXw"}},{"cell_type":"code","source":["# model definition\n","net = tf.keras.models.Sequential([\n","    # input layer                               \n","    tf.keras.layers.Flatten(),\n","    # hidden layer (number of units, activation function)\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    # output layer (number of output classes)\n","    tf.keras.layers.Dense(10)])"],"metadata":{"id":"3I4O3VaxGJkc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model and training hyperparameters"],"metadata":{"id":"2_Q6EgqTHppl"}},{"cell_type":"code","source":["# training hyperparameters\n","batch_size, lr, num_epochs = 256, 0.1, 10\n","# model hyperparameters\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","trainer = tf.keras.optimizers.SGD(learning_rate=lr)"],"metadata":{"id":"nEbN4ed5Hp_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Training the model"],"metadata":{"id":"Ib2peHqHHrIY"}},{"cell_type":"code","source":["d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"],"metadata":{"id":"Zh2WcLxjHrQc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluating the model"],"metadata":{"id":"OUso0dP-Hwj6"}},{"cell_type":"code","source":["d2l.predict_ch3(net, test_iter)"],"metadata":{"id":"NQ-_MVghHwrq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":30,"tab":["tensorflow"],"id":"SGUe7lsCatk5"},"source":["## Summary of the second approach\n","\n","* Using high-level APIs, we can implement MLPs much more concisely.\n","* For the same classification problem, the implementation of an MLP is the same as that of softmax regression except for additional hidden layers with activation functions.\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"name":"w02_MLP-FashionMNIST.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}