{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "oPR4ivrheD8_"
      },
      "source": [
        "# Logistic (softmax) regression model\n",
        "\n",
        "This notebook is organised in two parts:\n",
        "\n",
        "* the *first part* implements a **logistic regression model from scratch**, as we did for the linear regression model.\n",
        "\n",
        "* the *second part* makes use of **high-level APIs** for a concise implementation of the same logistic regression model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note for ST456\n",
        "\n",
        "The following commands are necessary for downloading some helper functions in TensorFlow used by the reference book.\n",
        "\n",
        "If you get a message saying **you need to restart the runtime**, please **do so before** running the rest of the code."
      ],
      "metadata": {
        "id": "TJPYO8trsP6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/d2l\n",
        "! wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/d2l/tensorflow.py\n",
        "!mv tensorflow.py /content/d2l\n",
        "\n",
        "#!pip install d2l==0.17.1 2>/dev/null"
      ],
      "metadata": {
        "id": "vxZJpnn1YKnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "tensorflow"
        ],
        "id": "VmKEfc0GeD9B"
      },
      "outputs": [],
      "source": [
        "# importing necessary modules\n",
        "import tensorflow as tf\n",
        "from IPython import display\n",
        "from d2l import tensorflow as d2l # helper module from the reference book"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "\n",
        "This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Here, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. \n",
        "\n",
        "You can access/import the Fashion MNIST directly from TensorFlow. Here, we are setting up a data iterator with batch size 256."
      ],
      "metadata": {
        "id": "gt8LgjadxQLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 4,
        "tab": [
          "tensorflow"
        ],
        "id": "IJr8ZZAFeD9C"
      },
      "outputs": [],
      "source": [
        "# loading data in 256 samples\n",
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First approach: logistic regression from scratch\n",
        "\n"
      ],
      "metadata": {
        "id": "tK9PPI1yzYYf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 5,
        "id": "ObtX16W3eD9D"
      },
      "source": [
        "## Initializing model parameters\n",
        "\n",
        "As in our linear regression example,\n",
        "each example here will be represented by a fixed-length vector.\n",
        "Each example in the raw dataset is a $28 \\times 28$ image.\n",
        "**We will flatten each image,\n",
        "treating them as vectors of length 784**, so each pixel will be treated as just another feature.\n",
        "\n",
        "Recall that in softmax regression,\n",
        "we have as many outputs as there are classes.\n",
        "**Because our dataset has 10 classes,\n",
        "our network will have an output dimension of 10**.\n",
        "Consequently, our weights will constitute a $784 \\times 10$ matrix\n",
        "and the biases will constitute a $1 \\times 10$ row vector.\n",
        "As with linear regression, we will initialize our weights `W`\n",
        "with Gaussian noise and our biases to take the initial value 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 8,
        "tab": [
          "tensorflow"
        ],
        "id": "kYir3G_QeD9D"
      },
      "outputs": [],
      "source": [
        "# input and output dimensions\n",
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "\n",
        "# initialising model parameters\n",
        "W = tf.Variable(tf.random.normal(shape=(num_inputs, num_outputs), mean=0, stddev=0.01))\n",
        "b = tf.Variable(tf.zeros(num_outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 9,
        "id": "m-5JIhAIeD9E"
      },
      "source": [
        "### Defining the softmax operation\n",
        "\n",
        "Before implementing the softmax regression model,\n",
        "let us briefly review how the sum operator works\n",
        "along specific dimensions in a tensor:\n",
        "\n",
        "**Given a matrix `X` we can sum over all elements (by default) or only\n",
        "over elements in the same axis**,\n",
        "i.e., the same column (axis 0) or the same row (axis 1).\n",
        "\n",
        "Note that if `X` is a tensor with shape (2, 3)\n",
        "and we sum over the columns,\n",
        "the result will be a vector with shape (3,).\n",
        "When invoking the sum operator,\n",
        "we can specify to keep the number of axes in the original tensor,\n",
        "rather than collapsing out the dimension that we summed over.\n",
        "This will result in a two-dimensional tensor with shape (1, 3).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "tensorflow"
        ],
        "id": "p7SD561ReD9E"
      },
      "outputs": [],
      "source": [
        "# example of sum operator over a tensor\n",
        "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "tf.reduce_sum(X, 0, keepdims=True), tf.reduce_sum(X, 1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "WzXU5HJdeD9G"
      },
      "source": [
        "We are now ready to **implement the softmax operation**.\n",
        "Recall that softmax consists of three steps:\n",
        "\n",
        "1. we exponentiate each term (using `exp`);\n",
        "\n",
        "2. we sum over each row (we have one row per example in the batch)\n",
        "to get the normalization constant for each example;\n",
        "\n",
        "3. we divide each row by its normalization constant,\n",
        "ensuring that the result sums to 1.\n",
        "\n",
        "Before looking at the code, let us recall\n",
        "how this looks expressed as an equation:\n",
        "\n",
        "\n",
        "$$\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(\\mathbf{X}_{ij})}{\\sum_k \\exp(\\mathbf{X}_{ik})}$$\n",
        "\n",
        "The denominator, or normalization constant,\n",
        "is also sometimes called the *partition function*\n",
        "(and its logarithm is called the log-partition function).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 13,
        "tab": [
          "tensorflow"
        ],
        "id": "kixTd3SieD9H"
      },
      "outputs": [],
      "source": [
        "# custom implementation of the softmax function\n",
        "def softmax(X):\n",
        "    X_exp = tf.exp(X)\n",
        "    partition = tf.reduce_sum(X_exp, 1, keepdims=True)\n",
        "    return X_exp / partition  # the broadcasting mechanism is applied here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 15,
        "id": "E-mGYYpyeD9H"
      },
      "source": [
        "As you can see, for any random input, **we turn each element into a non-negative number. Moreover, each row sums up to 1**, as is required for a probability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 17,
        "tab": [
          "tensorflow"
        ],
        "id": "VN5XPYlpeD9H"
      },
      "outputs": [],
      "source": [
        "X = tf.random.normal((2, 5), 0, 1)\n",
        "X_prob = softmax(X)\n",
        "X_prob, tf.reduce_sum(X_prob, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 18,
        "id": "wfBfv7dqeD9I"
      },
      "source": [
        "Note that while this looks correct mathematically,\n",
        "we were a bit sloppy in our implementation\n",
        "because we failed to take precautions against numerical overflow or underflow\n",
        "due to large or very small elements of the matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### Defining the model\n",
        "\n",
        "Now that we have defined the softmax operation,\n",
        "we can **implement the softmax regression model**.\n",
        "The below code defines how the input is mapped to the output through the network.\n",
        "Note that we flatten each original image in the batch\n",
        "into a vector using the `reshape` function\n",
        "before passing the data through our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 19,
        "tab": [
          "tensorflow"
        ],
        "id": "AoCpDrqyeD9I"
      },
      "outputs": [],
      "source": [
        "# custom logistic regression model\n",
        "def net(X):\n",
        "    return softmax(tf.matmul(tf.reshape(X, (-1, W.shape[0])), W) + b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 20,
        "id": "gd4mHzWGeD9I"
      },
      "source": [
        "### Defining the loss function\n",
        "\n",
        "Next, we need to implement the **cross-entropy loss function**. This may be the most common loss function\n",
        "in all of deep learning because, at the moment,\n",
        "classification problems far outnumber regression problems.\n",
        "\n",
        "Recall that cross-entropy takes the negative log-likelihood\n",
        "of the predicted probability assigned to the true label.\n",
        "Rather than iterating over the predictions with a Python for-loop\n",
        "(which tends to be inefficient),\n",
        "we can pick all elements by a single operator.\n",
        "\n",
        "Below, we **create sample data `y_hat`\n",
        "with 2 examples of predicted probabilities over 3 classes and their corresponding labels `y`**. With `y` we know that in the first example the first class is the correct prediction and\n",
        "in the second example the third class is the ground-truth.\n",
        "**Using `y` as the indices of the probabilities in `y_hat`**, we pick the probability of the first class in the first example\n",
        "and the probability of the third class in the second example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 22,
        "tab": [
          "tensorflow"
        ],
        "id": "uhKMD3ZNeD9J"
      },
      "outputs": [],
      "source": [
        "# example data for demonstrating the cross-entropy loss and the accuracy\n",
        "y_hat = tf.constant([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
        "y = tf.constant([0, 2])\n",
        "tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 23,
        "id": "PVIn-R7IeD9J"
      },
      "source": [
        "Now we can **implement the cross-entropy loss function** efficiently with just one line of code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "tensorflow"
        ],
        "id": "u-aaF0HgeD9J"
      },
      "outputs": [],
      "source": [
        "# custom implementation of the cross-entropy function\n",
        "def cross_entropy(y_hat, y):\n",
        "    return -tf.math.log(\n",
        "        tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1])))\n",
        "\n",
        "cross_entropy(y_hat, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 26,
        "id": "57fHXpyTeD9J"
      },
      "source": [
        "### Classification accuracy\n",
        "\n",
        "Given the predicted probability distribution `y_hat`,\n",
        "we typically choose the class with the highest predicted probability\n",
        "whenever we must output a hard prediction.\n",
        "Indeed, many applications require that we make a choice.\n",
        "Gmail must categorize an email into \"Primary\", \"Social\", \"Updates\", or \"Forums\".\n",
        "It might estimate probabilities internally,\n",
        "but at the end of the day it has to choose one among the classes.\n",
        "\n",
        "When predictions are consistent with the label class `y`, they are correct.\n",
        "**The classification accuracy is the fraction of all predictions that are correct**.\n",
        "Although it can be difficult to optimize accuracy directly (it is not differentiable),\n",
        "it is often the performance measure that we care most about,\n",
        "and we will nearly always report it when training classifiers.\n",
        "\n",
        "To compute accuracy we do the following:\n",
        "\n",
        "1. if `y_hat` is a matrix, we assume that the second dimension stores prediction scores for each class.\n",
        "\n",
        "2. we use `argmax` to obtain the predicted class by the index for the largest entry in each row.\n",
        "\n",
        "3. then we **compare the predicted class with the ground-truth `y` elementwise**. Since the equality operator `==` is sensitive to data types, we convert `y_hat`'s data type to match that of `y`.\n",
        "\n",
        "4. the result is a tensor containing entries of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 27,
        "tab": [
          "tensorflow"
        ],
        "id": "G_NFa687eD9K"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_hat, y):  \n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = tf.argmax(y_hat, axis=1)\n",
        "    cmp = tf.cast(y_hat, y.dtype) == y\n",
        "    return float(tf.reduce_sum(tf.cast(cmp, y.dtype)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 28,
        "id": "kMgvHqnheD9K"
      },
      "source": [
        "We will continue to use the variables `y_hat` and `y`\n",
        "defined before\n",
        "as the predicted probability distributions and labels, respectively.\n",
        "We can see that the first example's prediction class is 2\n",
        "(the largest element of the row is 0.6 with the index 2),\n",
        "which is inconsistent with the actual label, 0.\n",
        "The second example's prediction class is 2\n",
        "(the largest element of the row is 0.5 with the index of 2),\n",
        "which is consistent with the actual label, 2.\n",
        "Therefore, the classification accuracy rate for these two examples is 0.5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 29,
        "tab": [
          "tensorflow"
        ],
        "id": "O2taCRfTeD9L"
      },
      "outputs": [],
      "source": [
        "# example case\n",
        "accuracy(y_hat, y) / len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 30,
        "id": "weyNW6cseD9L"
      },
      "source": [
        "Similarly, we can **evaluate the accuracy for any model `net` on a dataset**] that is accessed via the data iterator `data_iter`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 31,
        "tab": [
          "tensorflow"
        ],
        "id": "Cy6ZnpLfeD9L"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy(net, data_iter): \n",
        "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
        "    for X, y in data_iter:\n",
        "        metric.add(accuracy(net(X), y), tf.size(y).numpy())\n",
        "    return metric[0] / metric[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 33,
        "id": "C9QMX98NeD9L"
      },
      "source": [
        "Here `Accumulator` is a utility class to accumulate sums over multiple variables.\n",
        "In the above `evaluate_accuracy` function,\n",
        "we create 2 variables in the `Accumulator` instance for storing both\n",
        "the number of correct predictions and the number of predictions, respectively.\n",
        "Both will be accumulated over time as we iterate over the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 34,
        "tab": [
          "tensorflow"
        ],
        "id": "krEH1haeeD9L"
      },
      "outputs": [],
      "source": [
        "# helper class for storing the number of correct predictions and the total number of predictions\n",
        "class Accumulator:  \n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 35,
        "id": "70JGP6SQeD9M"
      },
      "source": [
        "**Because we initialized the `net` model with random weights,\n",
        "the accuracy of this model should be close to random guessing**, i.e., 0.1 for 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 36,
        "tab": [
          "tensorflow"
        ],
        "id": "-YKB7w4keD9M"
      },
      "outputs": [],
      "source": [
        "evaluate_accuracy(net, test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 37,
        "id": "Ods_BjGheD9M"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "The training loop for softmax regression should look similar to our implementation of linear regression.\n",
        "Here we refactor the implementation to make it reusable.\n",
        "First, we define a function to train for one epoch.\n",
        "Note that `updater` is a general function to update the model parameters,\n",
        "which accepts the batch size as an argument.\n",
        "It can be either a wrapper of the `d2l.sgd` function\n",
        "or a framework's built-in optimization function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 40,
        "tab": [
          "tensorflow"
        ],
        "id": "S0XQ5dXjeD9M"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net, train_iter, loss, updater):  \n",
        "    \"\"\"The training loop\"\"\"\n",
        "    # Sum of training loss, sum of training accuracy, no. of examples\n",
        "    metric = Accumulator(3)\n",
        "    for X, y in train_iter:\n",
        "        # Compute gradients and update parameters\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_hat = net(X)\n",
        "            # Keras implementations for loss takes (labels, predictions)\n",
        "            # instead of (predictions, labels) that users might implement,\n",
        "            # e.g. `cross_entropy` that we implemented above\n",
        "            if isinstance(loss, tf.keras.losses.Loss):\n",
        "                l = loss(y, y_hat)\n",
        "            else:\n",
        "                l = loss(y_hat, y)\n",
        "        if isinstance(updater, tf.keras.optimizers.Optimizer):\n",
        "            params = net.trainable_variables\n",
        "            grads = tape.gradient(l, params)\n",
        "            updater.apply_gradients(zip(grads, params))\n",
        "        else:\n",
        "            updater(X.shape[0], tape.gradient(l, updater.params))\n",
        "        # Keras loss by default returns the average loss in a batch\n",
        "        l_sum = l * float(tf.size(y)) if isinstance(\n",
        "            loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n",
        "        metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
        "    # Return training loss and training accuracy\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 41,
        "id": "6RdjgEvpeD9N"
      },
      "source": [
        "Before showing the implementation of the training function,\n",
        "we define *a utility class that plot data in animation*.\n",
        "This is just for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 42,
        "tab": [
          "tensorflow"
        ],
        "id": "jiTNZRR_eD9N"
      },
      "outputs": [],
      "source": [
        "# helper function for animated data visualisation\n",
        "class Animator:  \n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        d2l.use_svg_display()\n",
        "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [self.axes,]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: d2l.set_axes(self.axes[\n",
        "            0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 43,
        "id": "iJKTLR-FeD9N"
      },
      "source": [
        "The following training function then\n",
        "trains a model `net` on a training dataset accessed via `train_iter`\n",
        "for multiple epochs, which is specified by `num_epochs`.\n",
        "At the end of each epoch,\n",
        "the model is evaluated on a testing dataset accessed via `test_iter`.\n",
        "We will leverage the `Animator` class to visualize\n",
        "the training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 44,
        "tab": [
          "tensorflow"
        ],
        "id": "1pvuf8goeD9O"
      },
      "outputs": [],
      "source": [
        "def train_model(net, train_iter, test_iter, loss, num_epochs, updater):  \n",
        "    \"\"\"Train a model\"\"\"\n",
        "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
        "                        legend=['train loss', 'train acc', 'test acc'])\n",
        "    for epoch in range(num_epochs):\n",
        "        # training step\n",
        "        train_metrics = train_epoch(net, train_iter, loss, updater)\n",
        "        # evaluation step\n",
        "        test_acc = evaluate_accuracy(net, test_iter)\n",
        "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
        "    train_loss, train_acc = train_metrics\n",
        "    assert train_loss < 0.5, train_loss\n",
        "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
        "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 45,
        "id": "ncrRogRKeD9O"
      },
      "source": [
        "As an implementation from scratch,\n",
        "we use the **minibatch stochastic gradient descent** to optimize the loss function of the model with a learning rate 0.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 47,
        "tab": [
          "tensorflow"
        ],
        "id": "P_KKu3h0eD9O"
      },
      "outputs": [],
      "source": [
        "# custom class for performing model optimisation\n",
        "class Updater():  \n",
        "    \"\"\"For updating parameters using minibatch stochastic gradient descent.\"\"\"\n",
        "    def __init__(self, params, lr):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "\n",
        "    def __call__(self, batch_size, grads):\n",
        "        d2l.sgd(self.params, grads, self.lr, batch_size)\n",
        "\n",
        "updater = Updater([W, b], lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 48,
        "id": "ueiUaEHjeD9O"
      },
      "source": [
        "Now we **train the model with 10 epochs**.\n",
        "Note that both the number of epochs (`num_epochs`),\n",
        "and learning rate (`lr`) are adjustable hyperparameters.\n",
        "By changing their values, we may be able\n",
        "to increase the classification accuracy of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 49,
        "tab": [
          "tensorflow"
        ],
        "id": "5lTzyW3BeD9P"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "train_model(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 50,
        "id": "PF-zAtq_eD9P"
      },
      "source": [
        "### Making predictions\n",
        "\n",
        "Now that training is complete,\n",
        "our model is ready to **classify some images**.\n",
        "Given a series of images,\n",
        "we will compare their actual labels\n",
        "(first line of text output)\n",
        "and the predictions from the model\n",
        "(second line of text output).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 51,
        "tab": [
          "tensorflow"
        ],
        "id": "apk95DxKeD9P"
      },
      "outputs": [],
      "source": [
        "def predict_image(net, test_iter, n=8):  \n",
        "    \"\"\"Predict labels\"\"\"\n",
        "    for X, y in test_iter:\n",
        "        break\n",
        "    trues = d2l.get_fashion_mnist_labels(y)\n",
        "    preds = d2l.get_fashion_mnist_labels(tf.argmax(net(X), axis=1))\n",
        "    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
        "    d2l.show_images(tf.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])\n",
        "\n",
        "predict_image(net, test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 52,
        "id": "Zy8hdPRfeD9Q"
      },
      "source": [
        "### Summary of the first approach\n",
        "\n",
        "* With softmax regression, we can train models for multiclass classification.\n",
        "* The training loop of softmax regression is very similar to that in linear regression: retrieve and read data, define models and loss functions, then train models using optimization algorithms. As you will soon find out, most common deep learning models have similar training procedures.\n",
        "\n",
        "---\n",
        "\n",
        "## Second approach: logist regression with `keras.Sequential` model\n",
        "\n",
        "### Initializing model parameters\n",
        "\n",
        "Remember that **the output layer of softmax regression\n",
        "is a fully-connected layer**. Therefore, to implement our model,\n",
        "we just need to add one fully-connected layer\n",
        "with 10 outputs to our `Sequential` model.\n",
        "Again, here, the `Sequential` is not really necessary,\n",
        "but we might as well form the habit since it will be ubiquitous\n",
        "when implementing deep models.\n",
        "Again, we initialize the weights at random\n",
        "with zero mean and standard deviation 0.01.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = tf.keras.models.Sequential()\n",
        "net.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
        "net.add(tf.keras.layers.Dense(10, kernel_initializer=weight_initializer))"
      ],
      "metadata": {
        "id": "8IMeb63B9wmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax implementation revisited\n",
        "\n",
        "In the previous approach, we calculated our model's output\n",
        "and then ran this output through the cross-entropy loss.\n",
        "Mathematically, that is a perfectly reasonable thing to do.\n",
        "However, from a computational perspective,\n",
        "exponentiation can be a source of numerical stability issues.\n",
        "\n",
        "Recall that the softmax function calculates\n",
        "$\\hat y_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$,\n",
        "where $\\hat y_j$ is the $j^\\mathrm{th}$ element of\n",
        "the predicted probability distribution $\\hat{\\mathbf{y}}$\n",
        "and $o_j$ is the $j^\\mathrm{th}$ element of the logits\n",
        "$\\mathbf{o}$.\n",
        "If some of the $o_k$ are very large (i.e., very positive),\n",
        "then $\\exp(o_k)$ might be larger than the largest number\n",
        "we can have for certain data types (i.e., *overflow*).\n",
        "This would make the denominator (and/or numerator) `inf` (infinity)\n",
        "and we wind up encountering either 0, `inf`, or `nan` (not a number) for $\\hat y_j$.\n",
        "In these situations we do not get a well-defined\n",
        "return value for cross-entropy.\n",
        "\n",
        "\n",
        "One trick to get around this is to first subtract $\\max(o_k)$\n",
        "from all $o_k$ before proceeding with the softmax calculation.\n",
        "You can see that this shifting of each $o_k$ by constant factor\n",
        "does not change the return value of softmax:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat y_j & =  \\frac{\\exp(o_j - \\max(o_k))\\exp(\\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))\\exp(\\max(o_k))} \\\\\n",
        "& = \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "After the subtraction and normalization step,\n",
        "it might be possible that some $o_j - \\max(o_k)$ have large negative values and thus that the corresponding $\\exp(o_j - \\max(o_k))$ will take values close to zero.\n",
        "These might be rounded to zero due to finite precision (i.e., *underflow*),\n",
        "making $\\hat y_j$ zero and giving us `-inf` for $\\log(\\hat y_j)$.\n",
        "A few steps down the road in backpropagation,\n",
        "we might find ourselves faced with a screenful\n",
        "of the dreaded `nan` results.\n",
        "\n",
        "Fortunately, we are saved by the fact that\n",
        "even though we are computing exponential functions,\n",
        "we ultimately intend to take their log\n",
        "(when calculating the cross-entropy loss).\n",
        "By combining these two operators\n",
        "softmax and cross-entropy together,\n",
        "we can escape the numerical stability issues\n",
        "that might otherwise plague us during backpropagation.\n",
        "As shown in the equation below, we avoid calculating $\\exp(o_j - \\max(o_k))$\n",
        "and can use instead $o_j - \\max(o_k)$ directly due to the canceling in $\\log(\\exp(\\cdot))$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log{(\\hat y_j)} & = \\log\\left( \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}\\right) \\\\\n",
        "& = \\log{(\\exp(o_j - \\max(o_k)))}-\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)} \\\\\n",
        "& = o_j - \\max(o_k) -\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We will want to keep the conventional softmax function handy\n",
        "in case we ever want to evaluate the output probabilities by our model.\n",
        "But instead of passing softmax probabilities into our new loss function,\n",
        "we will just **pass the logits and compute the softmax and its log\n",
        "all at once inside the cross-entropy loss function**."
      ],
      "metadata": {
        "id": "ey8_XB5DEDAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "Ti-A-k8w9wqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization algorithm\n",
        "\n",
        "Here, we **use minibatch stochastic gradient descent**\n",
        "with a learning rate of 0.1 as the optimization algorithm.\n",
        "Note that this is the same as we applied in the linear regression example and it illustrates the general applicability of the optimizers.\n"
      ],
      "metadata": {
        "id": "V1HKsZ_yE_GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = tf.keras.optimizers.SGD(learning_rate=.1)"
      ],
      "metadata": {
        "id": "8v7EvZCP9wtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "\n",
        "Next we **call the training function** to train the model."
      ],
      "metadata": {
        "id": "8MjK56-eFDG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "train_model(net, train_iter, test_iter, loss, num_epochs, trainer)"
      ],
      "metadata": {
        "id": "ElBsC4tr9wwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, this algorithm converges to a solution\n",
        "that achieves a decent accuracy,\n",
        "albeit this time with fewer lines of code than before.\n",
        "\n",
        "\n",
        "### Summary of the second approach\n",
        "\n",
        "* Using high-level APIs, we can implement softmax regression much more concisely.\n",
        "* From a computational perspective, implementing softmax regression has intricacies. Note that in many cases, a deep learning framework takes additional precautions beyond these most well-known tricks to ensure numerical stability, saving us from even more pitfalls that we would encounter if we tried to code all of our models from scratch in practice.\n"
      ],
      "metadata": {
        "id": "fYigQXUUFPnq"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "w01_softmax-regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}