{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w02_MLP_IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Binary classification of movie reviews\n",
        "\n",
        "Based on [Deep learning with Python](https://www.manning.com/books/deep-learning-with-python).\n",
        "\n",
        "This example applies a multilayer perceptron to classify movie reviews into \"positive\" and \"negative\" reviews, based on the text content of the reviews. \n",
        "\n",
        "We will be using the [IMDB dataset](https://keras.io/api/datasets/imdb/) packaged with Keras. This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations, such as: \"only consider the top 10,000 most common words\".\n",
        "\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n"
      ],
      "metadata": {
        "id": "gj2qHWvr89xy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOymSIuO6uJK"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.datasets import imdb\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from tensorflow.keras import optimizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset\n",
        "\n",
        "The argument `num_words=10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. \n",
        "\n",
        "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of `0`s and `1`s, where 0 stands for *negative* and 1 stands for *positive*."
      ],
      "metadata": {
        "id": "W1S-lDpa-HIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset and splitting into training and testing sets\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "id": "9YrqQCU8640v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory analysis"
      ],
      "metadata": {
        "id": "UmAUdgDm_FCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the training data\n",
        "train_data[0]"
      ],
      "metadata": {
        "id": "qOeh-8Jt7CER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the labels\n",
        "train_labels"
      ],
      "metadata": {
        "id": "WDj_sjYT7CG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the indices (limited to 10,000 words) for the training data \n",
        "max([max(sequence) for sequence in train_data])"
      ],
      "metadata": {
        "id": "AvBH_-eP7CJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are helper commands to decode reviews back to English words."
      ],
      "metadata": {
        "id": "g1Pa6O3p_ytz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word_index is a dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "# We reverse it, mapping integer indices to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# We decode the review; note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "metadata": {
        "id": "w8JIfN_j7CLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_review"
      ],
      "metadata": {
        "id": "wkiFfsnm_uAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data pre-processing\n",
        "\n",
        "We need to **transform the lists of integers into tensors**, so we can feed them into a neural network. \n",
        "\n",
        "There are different ways for doing that. Our approach will be to one-hot-encode our lists to turn them into vectors of 0s and 1s. For instance, a sequence `[3, 5]` will be encoded into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which will be ones. Then, the first layer of our network can be a `dense` layer capable of handling floating point vector data."
      ],
      "metadata": {
        "id": "sIJXX_sSBSEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for encoding the integer sequences into a binary matrix\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results"
      ],
      "metadata": {
        "id": "WWydT0bV7CN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "im52xH6DDEut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instance of the training data\n",
        "x_train[0]"
      ],
      "metadata": {
        "id": "gVWH7PkJ7U4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "AxN8H5Xw7U7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to monitor during training the accuracy of the model on data that it has never seen before, we will create a \"validation set\" by setting apart 10,000 samples from the original training data:"
      ],
      "metadata": {
        "id": "zseiL7pNFb0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting a validation sample for accuracy assessment\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "-xDa_-KR7VE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "5jL-kYuhDLYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model specification\n",
        "# we are using two layers with 16 hidden units each and 'relu' activation\n",
        "# also, an output layer with one hidden unit using 'sigmoid' activation to output \n",
        "# a probability indicating how likely the sample is to have a label 1\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "eX_smZJO7U-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model parameters\n",
        "# optimizer, loss function, and performance metric\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "caqzmTvk7VCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "\n",
        "We will train the model for 20 epochs (20 iterations over all samples in the `x_train` and `y_train` tensors), in mini-batches of 512 samples. We will keep track of loss and accuracy (`history` dictionary) on the 10,000 samples that we set apart, by passing the validation data as argument."
      ],
      "metadata": {
        "id": "ktXMvj3pF4Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # training the model \n",
        " # training parameters are: training features and labels, number of epochs, batch size, and validation data\n",
        " history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                     batch_size=512,\n",
        "                     validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "bN8MCeon7VHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the history object\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "MDhtQ5_N8jsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising model performance"
      ],
      "metadata": {
        "id": "1N5GqyNUHI9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting necessary data for plotting\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)"
      ],
      "metadata": {
        "id": "3h89k_n48juq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation loss\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yA1JUfKGKjKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation accuracy\n",
        "plt.clf()   # clear figure\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GL_7A1UM8jxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model (results) interpretation\n",
        "\n",
        "What is happening with this model?\n",
        "\n",
        "---\n",
        "\n",
        "Let's train a second model from scratch, for few epochs, and compare the results."
      ],
      "metadata": {
        "id": "uF2SmtREH8CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model parameters\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# training parameters\n",
        "history2 = model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "UHEljO2d8jzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the final results"
      ],
      "metadata": {
        "id": "ONNO_hROIsSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "_3DTeWkq8j1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation loss\n",
        "acc = history2.history['accuracy']\n",
        "loss = history2.history['loss']\n",
        "epochs = range(1, len(acc) + 1)"
      ],
      "metadata": {
        "id": "TyEdXbjqIslQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.title('Training loss - new model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o_9bEQEeKs7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation accuracy\n",
        "plt.clf()   # clear figure\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.title('Training accuracy - new model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dhRXJ49AJCKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions\n",
        "\n",
        "After having trained a network, we can use it to generate the likelihood of reviews being positive by using the `predict` method:"
      ],
      "metadata": {
        "id": "iWpuwP82KxrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x_test)"
      ],
      "metadata": {
        "id": "L72bkdB38j4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2Ma3R-sGMVTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}