{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK37AnAdpZI6"
      },
      "source": [
        "## CIFAR-10 Photo Classification\n",
        "\n",
        "This example is based on the following references:\n",
        "\n",
        "* [CIFAR 10](https://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/)\n",
        "* [CIFAR-10 from scratch](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/)\n",
        "\n",
        "---\n",
        "\n",
        "### CIFAR-10 DATASET\n",
        "\n",
        "CIFAR is an acronym that stands for the *Canadian Institute For Advanced Research* and the `CIFAR-10` dataset was developed along with the `CIFAR-100` dataset by researchers at the CIFAR institute.\n",
        "\n",
        "The dataset is comprised of 60,000 32×32 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, etc. The class labels and their standard associated integer values are listed below.\n",
        "\n",
        "- 0: airplane\n",
        "- 1: automobile\n",
        "- 2: bird\n",
        "- 3: cat\n",
        "- 4: deer\n",
        "- 5: dog\n",
        "- 6: frog\n",
        "- 7: horse\n",
        "- 8: ship\n",
        "- 9: truck\n",
        "\n",
        "These are very small images, much smaller than a typical photograph, and the dataset was intended for computer vision research.\n",
        "\n",
        "`CIFAR-10` is a well-understood dataset and widely used for benchmarking computer vision algorithms in the field of machine learning. The problem is “solved.” It is relatively straightforward to achieve 80% classification accuracy. Top performance on the problem is achieved by deep learning convolutional neural networks with a classification accuracy above 90% on the test dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7IajW1ivzUW"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsCvQ-3r3mpd"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQsQ-ywfn2P_"
      },
      "outputs": [],
      "source": [
        "# load embedded data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc0cKvXH2cIp"
      },
      "outputs": [],
      "source": [
        "# some exploratory analysis\n",
        "# create a grid of 3x3 images (rescaled for better visualisation)\n",
        "for i in range(0, 9):\n",
        "\tplt.subplot(330 + 1 + i)\n",
        "\tplt.imshow(X_train[i])\n",
        " \n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2u9FXBUuwRL"
      },
      "outputs": [],
      "source": [
        "assert X_train.shape == (50000, 32, 32, 3)\n",
        "assert X_test.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 1)\n",
        "assert y_test.shape == (10000, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC9HKjtwEyGX"
      },
      "source": [
        "The pixel values are in the range of 0 to 255 for each of the red, green and blue channels.\n",
        "\n",
        "--- \n",
        "\n",
        "## Data preprocessing\n",
        "\n",
        "It is good practice to work with normalized data. Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation which is 255.\n",
        "\n",
        "Note, the data is loaded as integers, so we must cast it to floating point values in order to perform the division."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gKsIqYFvsQY"
      },
      "outputs": [],
      "source": [
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHYU45ckE3cZ"
      },
      "source": [
        "We can use a one hot encoding to transform them into a binary matrix in order to best model the classification problem. We know there are 10 classes for this problem, so we can expect the binary matrix to have a width of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJNLLAlwvsTt"
      },
      "outputs": [],
      "source": [
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxauvv1q3dvc"
      },
      "source": [
        "## Model definition and training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G50feQUt3Zn2"
      },
      "outputs": [],
      "source": [
        "# general model definition and training parameters\n",
        "epochs = 25\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7IBBMfREZbi"
      },
      "source": [
        "## Baseline model\n",
        "\n",
        "Let’s start off by defining a simple CNN structure as a baseline and evaluate how well it performs on the problem.\n",
        "\n",
        "We will use a structure with two convolutional layers followed by max pooling and a flattening out of the network to fully connected layers to make predictions.\n",
        "\n",
        "Our baseline network structure can be summarized as follows:\n",
        "\n",
        "* Convolutional input layer, 32 feature maps with a size of 3×3, a rectifier activation function and a `kernel_initializer` function.\n",
        "* Convolutional layer, 32 feature maps with a size of 3×3, a rectifier activation function and a `kernel_initializer` function.\n",
        "* Max Pool layer with size 2×2.\n",
        "* Flatten layer.\n",
        "* Fully connected layer with 128 units and, a rectifier activation function and a `kernel_initializer` function.\n",
        "* Fully connected output layer with 10 units and a softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm0k4ys_vsVS"
      },
      "outputs": [],
      "source": [
        "# Create the baseline model - 1-block VGG architecture\n",
        "model1 = Sequential()\n",
        "model1.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', activation='relu', kernel_initializer='he_uniform'))\n",
        "model1.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) # changed from 512\n",
        "model1.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "sgd = SGD(learning_rate=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "print(model1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgUGwFPXvsZP"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "history = model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGFVeEtpxoOU"
      },
      "outputs": [],
      "source": [
        "# Final evaluation of the model\n",
        "scores = model1.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtFDhbb1DtXS"
      },
      "outputs": [],
      "source": [
        "# plot diagnostic learning curves\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='testing')\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='testing')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruaY4i3_4926"
      },
      "source": [
        "## Model improvement\n",
        "\n",
        "We have seen that a simple CNN performs poorly on this complex problem. In this section we look at scaling up the size and complexity of our model.\n",
        "\n",
        "Let’s design a deep version of the simple CNN above. We can introduce an additional round of convolutions with many more feature maps. We will use the same pattern of Convolutional, Convolutional and Max Pooling layers.\n",
        "\n",
        "This pattern will be repeated 2 times with 32 and 64 feature maps. The effect be an increasing number of feature maps with a smaller and smaller size given the max pooling layers. Finally an additional Dense layer will be used at the output end of the network in an attempt to better translate the large number feature maps to class values.\n",
        "\n",
        "We can summarize a new network architecture as follows:\n",
        "\n",
        "* Convolutional input layer, 32 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "* Convolutional layer, 32 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "* Max Pool layer with size 2×2.\n",
        "* Convolutional layer, 64 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "* Convolutional layer, 64 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "* Max Pool layer with size 2×2.\n",
        "* Flatten layer.\n",
        "* Fully connected layer with 128 units and a rectifier activation function.\n",
        "* Fully connected output layer with 10 units and a softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOdIWyAnvscX"
      },
      "outputs": [],
      "source": [
        "# Create the model - 2-block VGG architecture\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model2.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model2.add(MaxPooling2D())\n",
        "model2.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model2.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model2.add(MaxPooling2D())\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model2.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "sgd = SGD(learning_rate=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GLhGT3F5lkV"
      },
      "outputs": [],
      "source": [
        "history = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgDBW6Dd-rR6"
      },
      "outputs": [],
      "source": [
        "# Final evaluation of the model\n",
        "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3iUcFHh5lnH"
      },
      "outputs": [],
      "source": [
        "# plot diagnostic learning curves\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='testing')\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# save plot to file\n",
        "#import sys\n",
        "#filename = sys.argv[0].split('/')[-1]\n",
        "#plt.savefig(filename + '_plot.png')\n",
        "#plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (A bit more of) Model improvement\n",
        "\n",
        "We extend the previous model by adding a third VGG block."
      ],
      "metadata": {
        "id": "DOkIE9FiTulF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model - 3-block VGG architecture\n",
        "model3 = Sequential()\n",
        "model3.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(MaxPooling2D())\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(MaxPooling2D())\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(MaxPooling2D())\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model3.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "sgd = SGD(learning_rate=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model3.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "1MTzKAUCTt2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "e2Jv4YuCTt5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = model3.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "id": "bKyCPPRATt7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot diagnostic learning curves\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='testing')\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DmaQFuAFTt9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0DfD37CF-JY"
      },
      "source": [
        "## Extensions to improve model performance\n",
        "\n",
        "We have achieved good results on this very difficult problem, but we are still a good way from achieving world class results.\n",
        "\n",
        "Below are some ideas that you can try to extend upon the models and improve model performance.\n",
        "\n",
        "* **Regularization**. The first approach would be adding `Dropout` layers to the previous model. You can redesign `model1` and `model2` and add dropout after the first convolutional layer in each block, and also before each dense layer. This should address overfitting and improve models' performance. Another strategy would be **weight decay** (see below).\n",
        "\n",
        "* **Increase batch sizes**: You can also increase the batch size (currently 64) and check how this influence each model's performance.\n",
        "\n",
        "* **Train for more epochs**. Each model was trained for a very small number of epochs, 25. It is common to train large convolutional neural networks for hundreds or thousands of epochs. I would expect that performance gains can be achieved by significantly raising the number of training epochs.\n",
        "\n",
        "* **Image data augmentation**. The objects in the image vary in their position. Another boost in model performance can likely be achieved by using some data augmentation. Methods such as standardization and random shifts and horizontal image flips may be beneficial.\n",
        "\n",
        "* **Deeper network topology**. The larger network presented is deep, but larger networks could be designed for the problem. This may involve more feature maps closer to the input and perhaps less aggressive pooling. Additionally, standard convolutional network topologies that have been shown useful may be adopted and evaluated on the problem.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Dropout regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK8fO5pP5lw4"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model3 = Sequential()\n",
        "model3.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(MaxPooling2D())\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(MaxPooling2D())\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
        "model3.add(MaxPooling2D())\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "sgd = SGD(learning_rate=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model3.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaOYbITB_E83"
      },
      "source": [
        "### Larger batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_4RDMSCvsep"
      },
      "outputs": [],
      "source": [
        "history = model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m09bmzIsvsg8"
      },
      "outputs": [],
      "source": [
        "# Final evaluation of the model\n",
        "scores = model3.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLT-qZWyvcQ9"
      },
      "outputs": [],
      "source": [
        "# plot diagnostic learning curves\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pG8aGL__aV2"
      },
      "source": [
        "## Student's activity (self-study)\n",
        "\n",
        "### Weight decay\n",
        "\n",
        "Weight regularization or weight decay involves updating the loss function to penalize the model in proportion to the size of the model weights.\n",
        "\n",
        "This has a regularizing effect, as larger weights result in a more complex and less stable model, whereas smaller weights are often more stable and more general.\n",
        "\n",
        "We can add weight regularization to the convolutional layers and the fully connected layers by defining the `kernel_regularizer` argument and specifying the type of regularization. In this case, we will use `L2 weight regularization`, the most common type used for neural networks and a sensible default weighting of `0.001`. \n",
        "\n",
        "Notice that we are also using a `kernel_initializer` parameter.\n",
        "\n",
        "```\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4XLsE5RA-zV"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "## 1. expand model3 to include kernel_initializer and kernel_regularizer, \n",
        "## 2. train the model using the same parameters we have used in the last training, and\n",
        "## 3. check performance and plot the curves.\n",
        "\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Create the model\n",
        "\n",
        "# Compile model\n",
        "\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "zRLhltOuP01L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n"
      ],
      "metadata": {
        "id": "cEcj3958P03w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot diagnostic learning curves\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QUyoDlFIP9F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ4eDYvHBjfV"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "Data augmentation involves making copies of the examples in the training dataset with small random modifications.\n",
        "\n",
        "This has a regularizing effect as it both expands the training dataset and allows the model to learn the same general features, although in a more generalized manner.\n",
        "\n",
        "There are many types of data augmentation that could be applied. Given that the dataset is comprised of small photos of objects, we do not want to use augmentation that distorts the images too much, so that useful features in the images can be preserved and used.\n",
        "\n",
        "The types of random augmentations that could be useful include a horizontal flip, minor shifts of the image, and perhaps small zooming or cropping of the image.\n",
        "\n",
        "We will investigate the effect of simple augmentation on the baseline image, specifically horizontal flips and 10% shifts in the height and width of the image.\n",
        "\n",
        "This can be implemented in Keras using the `ImageDataGenerator` class. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksb1pI8vBmrg"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# create data generator\n",
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "# prepare iterator\n",
        "it_train = datagen.flow(X_train, y_train, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5XtmOhREyrU"
      },
      "source": [
        "During training, we can pass the iterator to the `model.fit_generator()` function and defining the number of batches in a single epoch.\n",
        "\n",
        "Observe that our model is using **dropout + data augmentation + kernel regularization** (if you manage to complete the missing code in the previous subsection).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W1-GSgWCeED"
      },
      "outputs": [],
      "source": [
        "# fit model\n",
        "steps = int(X_train.shape[0] / 64)\n",
        "history = model3.fit_generator(it_train, steps_per_epoch=steps, epochs=25, validation_data=(X_test, y_test), verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bkyJGLyDMPV"
      },
      "outputs": [],
      "source": [
        "# Final evaluation of the model\n",
        "scores = model3.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAG_-IP5DMSw"
      },
      "outputs": [],
      "source": [
        "# plot diagnostic learning curves\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sQq8sHfEbpN"
      },
      "source": [
        "## Further improvements\n",
        "\n",
        "### Variation of Dropout Regularization\n",
        "\n",
        "Dropout is working very well, so it may be worth investigating variations of how dropout is applied to the model.\n",
        "\n",
        "One variation that might be interesting is to **increase the amount of dropout from 20% to 25% or 30%**. Another variation that might be interesting is **using a pattern of increasing dropout from 20% for the first block, 30% for the second block, and so on to 50% at the fully connected layer** in the classifier part of the model.\n",
        "\n",
        "This type of increasing dropout with the depth of the model is a common pattern. It is effective as it forces layers deep in the model to regularize more than layers closer to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4fD5W7uDMX0"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "## modify your model to include variation of dropout rates and test again.\n",
        "\n",
        "# Create the model\n",
        "\n",
        "# Compile model\n",
        "\n",
        "model3.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using model.fit() and X_train, y_train data\n",
        "\n",
        "# If you want to experiment with data augmentation, uncomment the following lines\n",
        "#steps = int(X_train.shape[0] / 64)\n",
        "#history = model3.fit_generator(it_train, steps_per_epoch=steps, epochs=25, validation_data=(X_test, y_test), verbose=2)"
      ],
      "metadata": {
        "id": "alboA0auW4yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n"
      ],
      "metadata": {
        "id": "FQuZd3seW40u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot diagnostic learning curves\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "saQHPPZuW444"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUqgaO3QHY7v"
      },
      "source": [
        "### Dropout and Data Augmentation and Batch Normalization\n",
        "\n",
        "We can expand upon the previous example in a few ways.\n",
        "\n",
        "First, we can **increase the number of training epochs** from 25 to any other numbers (e.g. 100), to give the model more of an opportunity to improve.\n",
        "\n",
        "Next, we can **add batch normalization** in an effort to stabilize the learning and perhaps accelerate the learning process. This can be done by using the `model.add(BatchNormalization())` layer after each convolutional and dense layers (except the last dense layer). Have a look [here](https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/) for more details.\n",
        "\n",
        "To offset this acceleration, we can keep the regularization by changing the dropout from a fixed pattern to an increasing pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBzjxINDIH4R"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE\n",
        "## Update your model to include BatchNormalization layers and test again.\n",
        "\n",
        "# Create the model\n",
        "\n",
        "# Compile model\n",
        "\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using model.fit() and X_train, y_train data\n"
      ],
      "metadata": {
        "id": "CQH2JRqzbBGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n"
      ],
      "metadata": {
        "id": "b5EqzaGCbBJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot diagnostic learning curves\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='testing')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oibUPXgRbBNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making predcition with your final model\n",
        "\n",
        "After training and validating your model, you can ask for a prediction.\n",
        "\n",
        "Here we use a sample image from CIFAR-10 (a Deer, expected class 4) to ask for a prediction.\n",
        "\n",
        "**Make sure you upload `sample_image.png` to Google Colab**."
      ],
      "metadata": {
        "id": "jWjaoijvbqxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# load and prepare the image\n",
        "def load_image(filename):\n",
        "\t# load the image\n",
        "\timg = load_img(filename, target_size=(32, 32))\n",
        "\t# convert to array\n",
        "\timg = img_to_array(img)\n",
        "\t# reshape into a single sample with 3 channels\n",
        "\timg = img.reshape(1, 32, 32, 3)\n",
        "\t# prepare pixel data\n",
        "\timg = img.astype('float32')\n",
        "\timg = img / 255.0\n",
        "\treturn img"
      ],
      "metadata": {
        "id": "ljcOrO_NcMlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the image\n",
        "img = load_image('./sample_image.png')\n",
        "# predict the class\n",
        "result = model3.predict(img)\n",
        "# print the result\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "wFrqMNenchYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further improvements to the model\n",
        "\n",
        "You can experiment some further improvements to your model:\n",
        "\n",
        "* adding more Dense layers at the end, even increasing the number of units (from 128 to 1024 or 512).\n",
        "* training for a larger number of epochs.\n",
        "* experiment with different architectures, other than VGG."
      ],
      "metadata": {
        "id": "bidjhs-UyWO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HvTkov5lwKt4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "W05_CIFAR10_student_version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}