{"cells":[{"cell_type":"markdown","source":["# Digit classification using MNIST\n","\n","This is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the [MNIST homepage](http://yann.lecun.com/exdb/mnist/)."],"metadata":{"id":"w4y5Att8ekZB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"joCpi6VteNje"},"outputs":[],"source":["# importing necessary modules\n","import tensorflow as tf\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import matplotlib.cm as cm\n","%matplotlib inline\n","\n","# dataset used in this example\n","from keras.datasets import mnist\n","from keras.utils import np_utils\n","# layers used in this example\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Conv2D, MaxPool2D"]},{"cell_type":"markdown","source":["## Load the dataset\n","\n","* `x_train`: uint8 NumPy array of grayscale image data with shapes `(60000, 28, 28)`, containing the training data. Pixel values range from 0 to 255.\n","\n","* `y_train`: uint8 NumPy array of digit labels (integers in range 0-9) with shape `(60000,)` for the training data.\n","\n","* `x_test`: uint8 NumPy array of grayscale image data with shapes `(10000, 28, 28)`, containing the test data. Pixel values range from 0 to 255.\n","\n","* `y_test`: uint8 NumPy array of digit labels (integers in range 0-9) with shape `(10000,)` for the test data."],"metadata":{"id":"4360U9DkgJ0A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQoTXbcHeNjZ"},"outputs":[],"source":["# loading the dataset and spliting into training and testing samples\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# checking\n","assert X_train.shape == (60000, 28, 28)\n","assert X_test.shape == (10000, 28, 28)\n","assert y_train.shape == (60000,)\n","assert y_test.shape == (10000,)"]},{"cell_type":"markdown","source":["## Exploratory data analysis"],"metadata":{"id":"SxwOpM0yiGGA"}},{"cell_type":"code","source":["# how many instances in each training class\n","sns.countplot(y_train)"],"metadata":{"id":"6EoGJOVqievF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking for missing data\n","print(np.isnan(X_train).any(), np.isnan(X_test).any())"],"metadata":{"id":"DAlhn7HOi9Tn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1l8KF-TReNjf"},"outputs":[],"source":["# plot the first six training images\n","fig = plt.figure(figsize=(20,20))\n","for i in range(6):\n","    ax = fig.add_subplot(1, 6, i+1, xticks=[], yticks=[])\n","    ax.imshow(X_train[i]) # add cmap='gray' for gray scale images\n","    ax.set_title(str(y_train[i]))"]},{"cell_type":"markdown","source":["## Data pre-processing\n","\n","### Normalization\n","\n","Models generally run better on normalized values. The best way to normalize the data depends on each individual dataset. For the MNIST dataset, we want each value to be between 0.0 and 1.0. As all values originally fall under the 0.0-255.0 range, divide by 255.0."],"metadata":{"id":"xcQKXNkTjQIV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mq663XMReNjg"},"outputs":[],"source":["# rescale [0,225] --> [0,1]\n","X_train = X_train.astype('float32')/255\n","X_test = X_test.astype('float32')/255"]},{"cell_type":"markdown","source":["### Label Encoding\n","\n","The labels for the training and the testing dataset are currently categorical (not continuous). To include a categorical dataset in our model, the labels should be converted to one-hot encodings.\n","\n","For example, `2` becomes `[0,0,1,0,0,0,0,0,0,0]` and 7 becomes `[0,0,0,0,0,0,0,1,0,0]`."],"metadata":{"id":"O_Fadi76m6tr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PucIpUjBeNjh"},"outputs":[],"source":["# print first ten (integer valued) training labels\n","print('Integer-valued labels: ')\n","print(y_train[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdPf0JI_eNji"},"outputs":[],"source":["# one-hot encode the labels\n","y_train = np_utils.to_categorical(y_train, 10)\n","y_test = np_utils.to_categorical(y_test, 10)\n","\n","# print first ten (one hot) training labels\n","print('One-hot labels: ')\n","print(y_train[:10])"]},{"cell_type":"code","source":["plt.imshow(X_train[100][:,:])\n","print(\"One-hot encoded class label: \", y_train[100])"],"metadata":{"id":"zX9SZh0SnY-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Backup data \n","\n","For this exercise, we will testing a single model (Dense layer) first (as baseline), and then experimenting with more specialised models. \n","\n","For the baseline model, we need to reshape the input data. This will be `X_train` and `X_test` samples. All the remaining models will use a copy of these data samples (`X_train2` and `X_test2`), with the original shape."],"metadata":{"id":"BFe7LjMFUKIc"}},{"cell_type":"code","source":["# X_train2 and X_test2 will keep the original 3D tensor format (28,28,1) while X_train and X_test will be reshaped to a 1D tensor (784).\n","X_train2 = X_train\n","X_test2 = X_test"],"metadata":{"id":"M01ydXS_LBuD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## General model definition and training parameters"],"metadata":{"id":"XqD2RXg-wtAJ"}},{"cell_type":"code","source":["# Some useful parameters for model definition and training\n","batch_size = 64\n","num_classes = 10\n","epochs = 10"],"metadata":{"id":"lznJXVtfwxzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Baseline model: 1D model\n","\n","We can start with a single Dense layer model and check the performance."],"metadata":{"id":"TDLF2_bpJ8hV"}},{"cell_type":"code","source":["# we need to reshape the data, as the Dense layer expects a single 1D tensor as input\n","print(\"X_train original shape: \",  X_train.shape)\n","X_train = tf.reshape(X_train, [-1, 784])\n","print(\"X_train original shape: \", X_train.shape)\n","\n","print(\"X_test original shape: \",  X_test.shape)\n","X_test = tf.reshape(X_test, [-1, 784])\n","print(\"X_test original shape: \", X_test.shape)"],"metadata":{"id":"O_pqihwiBz7u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# uncomment this line if you want to see the image\n","# remember to upload the image (from GitHub) into Google Colab\n","#from IPython.display import display, Image\n","#display(Image(filename='/content/w05_baselineCNNmodel.png'))"],"metadata":{"id":"YtFATc02VMxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# baseline model - single Dense layer\n","model1 = tf.keras.Sequential(\n","  [\n","      tf.keras.layers.Input(shape=(28*28)), # input should be only the dense vector (784 data points)\n","      tf.keras.layers.Dense(num_classes, activation='softmax')\n","  ])\n","\n","model1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# print model layers\n","model1.summary()"],"metadata":{"id":"G6GmGFXLA5XM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%time\n","history = model1.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.3)"],"metadata":{"id":"WOW402qKBlJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate test accuracy\n","score = model1.evaluate(X_test, y_test, verbose=0)\n","accuracy = 100*score[1]\n","\n","# print test accuracy\n","print('Test accuracy %.2f%%' % accuracy)"],"metadata":{"id":"9X6F0o8YBlOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training Accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation Accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"metadata":{"id":"p1fBeLF7BlRc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Comments on the baseline model\n","\n","---\n","\n","### Second model\n","\n","We will add some convolutional and pooling layers to improve model's performance.\n","\n","* `Conv2D` layers are convolutions. Each filter (32 in this example) transforms a part of the image (5x5 in this example). The transformation is applied on the whole image.\n","\n","* `MaxPool2D` is a downsampling filter. It reduces a 2x2 matrix of the image to a single pixel with the maximum value of the 2x2 matrix. The filter aims to conserve the main features of the image while reducing the size.\n","\n","* `relu` is the rectifier, and it is used to find nonlinearity in the data. It works by returning the input value if the input value >= 0. If the input is negative, it returns 0.\n","\n","* `Flatten` converts the tensors into a 1D vector.\n","\n","* The `Dense` layer is a fully-connected neural network (ANN). The last layer returns the probability that an image is in each class (one for each digit).\n","\n","* As this model aims to categorize the images, we will use a `categorical_crossentropy` loss function."],"metadata":{"id":"onTImR9XuLUv"}},{"cell_type":"code","source":["model2 = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(28,28,1), name='1st_Conv2D'),\n","    tf.keras.layers.MaxPool2D(name='1st_MaxPool2D'),\n","    tf.keras.layers.Flatten(name='flatten'),\n","    tf.keras.layers.Dense(num_classes, activation='softmax', name='final_dense')\n","])\n","\n","model2.summary()"],"metadata":{"id":"ZIgshNI2uKz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZaSMfL_eNji"},"outputs":[],"source":["# compile the model\n","model2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) # changed from rmsprop"]},{"cell_type":"markdown","source":["### Training the model"],"metadata":{"id":"EL_Hd2Hr1iRu"}},{"cell_type":"code","source":["%time\n","history = model2.fit(X_train2, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.3)"],"metadata":{"id":"to9YFHJQqBrd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model evaluation"],"metadata":{"id":"f4yNUIzR1mlV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4OWqak8eNjj"},"outputs":[],"source":["# evaluate test accuracy\n","score = model2.evaluate(X_test2, y_test, verbose=0)\n","accuracy = 100*score[1]\n","\n","# print test accuracy\n","print('Test accuracy %.2f%%' % accuracy)"]},{"cell_type":"code","source":["history.history.keys()"],"metadata":{"id":"J5UofrIWtsTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training Accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation Accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"metadata":{"id":"HbnAQn62tUPf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Comments on the second model\n","\n","---\n","\n","### Third model\n","\n","In this third model, we are adding a second `Conv2D` layer with 64 3x3 kernels and keep the remaining parameters."],"metadata":{"id":"QjOQIySd16pe"}},{"cell_type":"code","source":["model3 = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(28,28,1)),\n","    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n","    tf.keras.layers.MaxPool2D(),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(num_classes, activation='softmax')\n","])\n","\n","model3.summary()"],"metadata":{"id":"ME8i1f_PtUZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) # changed from rmsprop"],"metadata":{"id":"XPSosb2gtUd4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%time\n","history = model3.fit(X_train2, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.3)"],"metadata":{"id":"yKBlP1K4tUhc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jE04q4-SeNjn"},"outputs":[],"source":["# evaluate test accuracy\n","score = model3.evaluate(X_test2, y_test, verbose=0)\n","accuracy = 100*score[1]\n","\n","# print test accuracy\n","print('Test accuracy %.4f%%' % accuracy)"]},{"cell_type":"code","source":["fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training Accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation Accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"metadata":{"id":"ZWR1fUZe2ut3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Comments on third model\n","\n","---\n","\n","### Optimization of the third model\n","\n","If we observe overfitting in the model, we can add `Dropout` as a regularization layer to prevent (or minimize) it. \n","\n","In this `model3d`, 25% of the nodes in the layer are randomly ignored in each epoch of training, allowing the network to learn different features."],"metadata":{"id":"cYuSqK0c9JKQ"}},{"cell_type":"code","source":["model3d = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=(28,28,1)),\n","    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n","    tf.keras.layers.MaxPool2D(),\n","    tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(num_classes, activation='softmax')\n","])\n","\n","model3d.summary()"],"metadata":{"id":"gy242NYM9JZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model3d.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) # changed from rmsprop"],"metadata":{"id":"JD4UPVSL9JdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Callback\n","\n","We can use the `ModelCheckpoint` callback function to keep track of model's parameters during training. The `save_best_only=True` option only saves when the model is considered the *best* and the latest best model according to the quantity monitored will not be overwritten. The best model is saved to a file and further loaded into a **new model** for performing prediction."],"metadata":{"id":"bDyqgbvUEzOY"}},{"cell_type":"code","source":["from keras.callbacks import ModelCheckpoint\n","\n","# training the model\n","checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5', verbose=1, save_best_only=True)"],"metadata":{"id":"ASNb87K6E6pL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also use a **larger batch size** for minimizing the training time."],"metadata":{"id":"LUIhxg5WHUja"}},{"cell_type":"code","source":["%time\n","history = model3d.fit(X_train2, y_train, batch_size=256, epochs=epochs, validation_split=0.3, callbacks=[checkpointer], shuffle=True)"],"metadata":{"id":"uu0PQg2D9JgK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate test accuracy\n","score = model3d.evaluate(X_test2, y_test, verbose=0)\n","accuracy = 100*score[1]"],"metadata":{"id":"Gz7ysxt39tWs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print test accuracy\n","print('Test accuracy %.2f%%' % accuracy)"],"metadata":{"id":"7gCmuRzx9tad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training Accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation Accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"metadata":{"id":"HwgrLIxG9teq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading the best model into a *new* model for prediction"],"metadata":{"id":"S2m65loNejPI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ck4kEGxeNjo"},"outputs":[],"source":["# load the weights that yielded the best validation accuracy\n","model3d.load_weights('mnist.model.best.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMBeSH7yeNjo"},"outputs":[],"source":["# evaluate test accuracy\n","score = model3d.evaluate(X_test2, y_test, verbose=0)\n","accuracy = 100*score[1]\n","\n","# print test accuracy\n","print('Test accuracy %.2f%%' % accuracy)"]},{"cell_type":"markdown","source":["### Students' activity (self-study)\n","\n","Try to improve the model by adding more layers and/or increasing the batch size. You can also rerun all the code and play with other optimizer (e.g. `rmsprop` or `adam`).\n","\n","Remember to use `X_train2` and `X_test2` as input data for training/validation and evaluation.\n","\n","The model is able to achieve >98% accuracy depending on your choices."],"metadata":{"id":"djweRecRQavw"}},{"cell_type":"markdown","source":["We can also use a larger batch size for minimizing the training time."],"metadata":{"id":"reFWtWwgQdbq"}},{"cell_type":"code","source":["## model definition\n","model4 = ..."],"metadata":{"id":"9j93M8ZxatLA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## training\n","## you can decide on using shuffle and the ModelCheckpoint callback we've used before\n","%time\n","history = model4.fit(...)"],"metadata":{"id":"9Khn5S9gQdbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate test accuracy\n","score = model4.evaluate(...)\n","accuracy = 100*score[1]"],"metadata":{"id":"NzFF3EbRQdbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print test accuracy\n","print('Test accuracy %.2f%%' % accuracy)"],"metadata":{"id":"SOVttCXYQdbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training Accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation Accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"metadata":{"id":"poGnEN65Qdbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"QscTK7NKjUNE"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"W05_CNN_MNIST_student_version.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}