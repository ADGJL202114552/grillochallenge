{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP_Reuters.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOV6f+vRCLJ8cEdRCBFAKuN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Multi-class classification: Reuters newswire\n","\n","This example is based on the [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition) book.\n","\n","We will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. This is an instance of *multi-class classification*, and since each data point should be classified into only one category, the problem is more specifically an instance of *single-label, multi-class classification*. \n"],"metadata":{"id":"RchdvhzKALH3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOymSIuO6uJK"},"outputs":[],"source":["# importing necessary libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from keras.datasets import reuters\n","from keras import models\n","from keras import layers"]},{"cell_type":"markdown","source":["### Loading the dataset\n","\n","We will be working with the [Keras' Reuters dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/reuters). It consists of 11,228 newswires labeled over 46 topics, originally generated by parsing and preprocessing the [classic Reuters-21578 dataset](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection) used for text categorization.\n","\n","There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\n"],"metadata":{"id":"xXFF5Y2e9hxq"}},{"cell_type":"code","source":["# loading training and testing data with the top-10,000 most frequent words\n","(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"],"metadata":{"id":"9YrqQCU8640v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We have 8,982 training examples and 2,246 test examples:"],"metadata":{"id":"LbIK63pfoPkU"}},{"cell_type":"code","source":["print(f'Training data instances: %d\\nTesting data instances: %d' % (len(train_data), len(test_data)))"],"metadata":{"id":"6DMj7-hk9t6J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data exploration and pre-processing\n","\n","As with the IMDB reviews, each newswire is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer `3` encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: *only consider the top 10,000 most common words, but eliminate the top 20 most common words*.\n","\n","As a convention, `0`[link text](https://) does not stand for a specific word, but instead is used to encode any unknown word."],"metadata":{"id":"xiVKFWnhoVfD"}},{"cell_type":"code","source":["train_data[10]"],"metadata":{"id":"qOeh-8Jt7CER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.shape"],"metadata":{"id":"rKVwuG0XGy5O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The label associated with an example is an integer between 0 and 45: a topic index.\n","\n","As reference, [here is a list of classes and labels](https://martin-thoma.com/nlp-reuters/#classes-and-labels), but not necessarily mapping the keras Reuters dataset.\n","\n","`topic 3`, for instance, is `money-fx`."],"metadata":{"id":"7FsFd41W-xyp"}},{"cell_type":"code","source":["train_labels[10]"],"metadata":{"id":"WDj_sjYT7CG3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Decoding some words (for exploratory analysis)."],"metadata":{"id":"G1CflvX1_aDh"}},{"cell_type":"code","source":["# word_index is a dictionary mapping words to an integer index\n","word_index = reuters.get_word_index()\n","# We reverse it, mapping integer indices to words\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","# We decode the review; note that our indices were offset by 3\n","# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n","decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[10]])"],"metadata":{"id":"w8JIfN_j7CLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoded_newswire"],"metadata":{"id":"NMeTcJtnoXiE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Vectorizing the data**.\n","\n","We can vectorize the training data."],"metadata":{"id":"31l7EwgPAsbE"}},{"cell_type":"code","source":["# custom function for data vectorization\n","def vectorize_sequences(sequences, dimension=10000):\n","    # create an all-zero matrix of shape (len(sequences), dimension)\n","    results = np.zeros((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n","    return results\n","\n","# vectorized training data\n","x_train = vectorize_sequences(train_data)\n","# vectorized testing data\n","x_test = vectorize_sequences(test_data)"],"metadata":{"id":"WWydT0bV7CN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train[0]"],"metadata":{"id":"gVWH7PkJ7U4_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a **one-hot encoding**, which is widely used format for categorical data (also called *categorical encoding*). \n","\n","In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index."],"metadata":{"id":"D3puRJWCB0YT"}},{"cell_type":"code","source":["def to_one_hot(labels, dimension=46):\n","    results = np.zeros((len(labels), dimension))\n","    for i, label in enumerate(labels):\n","        results[i, label] = 1.\n","    return results\n","\n","# vectorized training labels\n","one_hot_train_labels = to_one_hot(train_labels)\n","# vectorized testing labels\n","one_hot_test_labels = to_one_hot(test_labels)"],"metadata":{"id":"-Rn3qtdP-IFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alternative approach to previous cell\n","#from keras.utils.np_utils import to_categorical\n","#one_hot_train_labels = to_categorical(train_labels)\n","#one_hot_test_labels = to_categorical(test_labels)"],"metadata":{"id":"FjSE3wOf-PIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_hot_train_labels[0]"],"metadata":{"id":"UFT-zcTwCHTB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Validation data**\n","\n","We will set apart 1,000 samples from the training data to be used as *validation data*."],"metadata":{"id":"QukmJVl5JIkR"}},{"cell_type":"code","source":["x_val = x_train[:1000]\n","partial_x_train = x_train[1000:]\n","y_val = one_hot_train_labels[:1000]\n","partial_y_train = one_hot_train_labels[1000:]"],"metadata":{"id":"wumtJlwGJLZz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model definition\n","\n","When defining a model, two important aspects are related to **the number of layers to use** and **the dimensionality of each layer**.\n","\n","Remember that our model needs to generate outputs for all the 46 topics (classes); i.e., it needs to learn how to separate 46 different classes. This demands large layers - in our case, we will be using 64 hidden units."],"metadata":{"id":"ix2wm21uCOi5"}},{"cell_type":"code","source":["# Sequential model\n","model1 = models.Sequential()\n","model1.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n","model1.add(layers.Dense(64, activation='relu'))\n","# last layer with softmax activation function for generating a probability distribution over the 46 classes\n","model1.add(layers.Dense(46, activation='softmax'))\n","\n","model1.summary()"],"metadata":{"id":"eX_smZJO7U-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model hyperparameters**\n","\n"],"metadata":{"id":"egtbdUpwHhlb"}},{"cell_type":"code","source":["model1.compile(optimizer='rmsprop',\n","              # for multi-class classification problems \n","              # (with labels provided in a one-hot representation)\n","              loss='categorical_crossentropy', \n","              metrics=['accuracy'])"],"metadata":{"id":"caqzmTvk7VCm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training the model\n","\n","We will be training the model for 20 epochs and using a batch of 512 samples from the `partial_x/y_train` datasets. Then, we will validate the model over the `x/y_val` dataset."],"metadata":{"id":"7D60CHsTJb-v"}},{"cell_type":"code","source":["# training the model and keep track (history) of loss and accuracy\n","history = model1.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=20,\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val))"],"metadata":{"id":"bN8MCeon7VHS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualising the results"],"metadata":{"id":"y51-XsTQQPSD"}},{"cell_type":"code","source":["# checking which keys we have in the `history` dictionary\n","history_dict = history.history\n","history_dict.keys()"],"metadata":{"id":"MDhtQ5_N8jsF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(loss) + 1)\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"3h89k_n48juq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.clf()   # clear figure\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"GL_7A1UM8jxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**What we can draw from the observed loss and accuracy?**\n","\n","Do you notice any overfitting in this model? If yes, from what epoch?\n","\n","---\n","\n","**GROUP DISCUSSION**\n","\n","Play with different hyperparameters to improve the model performance. You can change: *number of epochs*, *optimizer*, and *number of hidden units**.\n","\n","A good starting point is, based on the previous results and in case you noticed overfitting, change the number of epochs to some number before the overfitting."],"metadata":{"id":"VRrPPVmaMotT"}},{"cell_type":"code","source":["# second model\n","model2 = models.Sequential()\n","model2.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n","model2.add(layers.Dense(64, activation='relu'))\n","model2.add(layers.Dense(46, activation='softmax'))\n","\n","# replace @OPT by any available optimizer\n","# https://keras.io/api/optimizers/\n","model2.compile(optimizer='@OPT',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# replace @EPO by the number of epochs you want to try\n","history2 = model2.fit(partial_x_train,\n","                     partial_y_train,\n","                     epochs=@EPO,\n","                     batch_size=512,\n","                     validation_data=(x_val, y_val))\n","\n","# evaluate the model and get the results\n","results = model2.evaluate(x_test, one_hot_test_labels)"],"metadata":{"id":"UHEljO2d8jzn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = history2.history['loss']\n","val_loss = history2.history['val_loss']\n","epochs = range(1, len(loss) + 1)\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"wqL1BesLPYeu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.clf()   # clear figure\n","\n","acc = history2.history['accuracy']\n","val_acc = history2.history['val_accuracy']\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ZBd1vWvGPYhV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the final results (loss, accuracy)\n","results"],"metadata":{"id":"_3DTeWkq8j1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OPTIONAL - comparing the model with a random baseline\n","#import copy\n","\n","#test_labels_copy = copy.copy(test_labels)\n","#np.random.shuffle(test_labels_copy)\n","#float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"],"metadata":{"id":"lhLPTNSx9Mrm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Making predictions\n","\n","We can compare both models (`model1` and `model2`)."],"metadata":{"id":"K9PUyZwjRr3I"}},{"cell_type":"code","source":["# asking predictions from both models\n","predictions1 = model1.predict(x_test)\n","predictions2 = model2.predict(x_test)"],"metadata":{"id":"eH4gLxe5_PkS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions1[0].shape"],"metadata":{"id":"546EGeOb_PmW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.sum(predictions1[0])"],"metadata":{"id":"pIshj3-J_Po-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.argmax(predictions1[0])"],"metadata":{"id":"9XOYcyYP_PrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions1[0]"],"metadata":{"id":"nlph2A5UUf0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show the inputs and predicted outputs\n","for i in range(3):\n","\tprint(\"X=%s, Predicted=%s\" % (x_test[i], predictions1[i]))"],"metadata":{"id":"-LfOcbqR_Pta"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**What happens if we decide for intermediate layers with few hidden units?**"],"metadata":{"id":"z9hXDe4VWP_7"}},{"cell_type":"code","source":["model3 = models.Sequential()\n","model3.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n","model3.add(layers.Dense(4, activation='relu'))\n","model3.add(layers.Dense(46, activation='softmax'))\n","\n","model3.compile(optimizer='rmsprop',\n","               loss='categorical_crossentropy',\n","               metrics=['accuracy'])\n","\n","model3.fit(partial_x_train,\n","           partial_y_train,\n","           epochs=20,\n","           batch_size=128,\n","           validation_data=(x_val, y_val))"],"metadata":{"id":"IUDetF7c_0Iu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = model.evaluate(x_test, one_hot_test_labels)"],"metadata":{"id":"3qUZjPlV_0LR"},"execution_count":null,"outputs":[]}]}