{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "Yqo_oN24a0Ut"
      },
      "source": [
        "# Linear regression model\n",
        "\n",
        "This notebook is organised in two parts:\n",
        "\n",
        "* the *first part* implements a **linear regression model from scratch**, including the data pipeline, the model, the loss function, and the minibatch stochastic gradient descent optimizer.\n",
        "\n",
        "* the *second part* makes use of **TensorFlow's high-level APIs** (`data`, `keras`, `initializers` etc) for a concise implementation of a linear regression model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note for ST456\n",
        "\n",
        "The following command is necessary for downloading some helper functions in TensorFlow used by the reference book.\n",
        "\n",
        "If you get a message saying **you need to restart the runtime**, please **do so** before running the rest of the code."
      ],
      "metadata": {
        "id": "8QRwLYIYce3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/d2l\n",
        "! wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/d2l/tensorflow.py\n",
        "!mv tensorflow.py /content/d2l\n",
        "\n",
        "#!pip install d2l==0.17.1 2>/dev/null"
      ],
      "metadata": {
        "id": "nOUMg5v3c2-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "tensorflow"
        ],
        "id": "_T8yCxPEa0Uz"
      },
      "outputs": [],
      "source": [
        "# importing necessary modules\n",
        "%matplotlib inline\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l  # helper module from the textbook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "L-V1FaEia0U1"
      },
      "source": [
        "## Generating the dataset\n",
        "\n",
        "To keep things simple, we will **construct an artificial dataset\n",
        "according to a linear model with additive noise.**\n",
        "Our task will be to recover this model's parameters\n",
        "using the finite set of examples contained in our dataset.\n",
        "We will keep the data low-dimensional so we can visualize it easily.\n",
        "\n",
        "In the following code snippet, we generate a dataset\n",
        "containing 1000 examples, each consisting of 2 features\n",
        "sampled from a standard normal distribution.\n",
        "Thus our synthetic dataset will be a matrix\n",
        "$\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$.\n",
        "\n",
        "**The true parameters generating our dataset will be\n",
        "$\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$**,\n",
        "and our synthetic labels will be assigned according\n",
        "to the following linear model with the noise term $\\epsilon$:\n",
        "\n",
        "**$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$$**\n",
        "\n",
        "You could think of $\\epsilon$ as capturing potential\n",
        "measurement errors on the features and labels.\n",
        "We will assume that the standard assumptions hold and thus\n",
        "that $\\epsilon$ obeys a normal distribution with mean of 0.\n",
        "To make our problem easy, we will set its standard deviation to 0.01.\n",
        "\n",
        "The following code generates our synthetic dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 6,
        "tab": [
          "tensorflow"
        ],
        "id": "wGd7cZuma0U2"
      },
      "outputs": [],
      "source": [
        "def synthetic_data(w, b, num_examples):  \n",
        "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
        "    X = tf.zeros((num_examples, w.shape[0]))\n",
        "    X += tf.random.normal(shape=X.shape)\n",
        "    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b\n",
        "    y += tf.random.normal(shape=y.shape, stddev=0.01)\n",
        "    y = tf.reshape(y, (-1, 1))\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 7,
        "tab": [
          "tensorflow"
        ],
        "id": "IfDg8Sapa0U3"
      },
      "outputs": [],
      "source": [
        "true_w = tf.constant([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "iFu7eIQma0U3"
      },
      "source": [
        "Note that **each row in `features` consists of a 2-dimensional data example\n",
        "and that each row in `labels` consists of a 1-dimensional label value (a scalar).**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 9,
        "tab": [
          "tensorflow"
        ],
        "id": "S26miURea0U4"
      },
      "outputs": [],
      "source": [
        "print('features:', features[0], '\\nlabel:', labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 10,
        "id": "u3UleroJa0U6"
      },
      "source": [
        "By generating a scatter plot using the second feature `features[:, 1]` and `labels`,\n",
        "we can clearly observe the linear correlation between the two.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "tensorflow"
        ],
        "id": "v3_esLxSa0U7"
      },
      "outputs": [],
      "source": [
        "d2l.set_figsize()\n",
        "# the semicolon is for displaying the plot only\n",
        "d2l.plt.scatter(features[:, (1)].numpy(), labels.numpy(), 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First approach: linear regression model from scratch\n",
        "\n",
        "\n",
        "`[based on the D2L reference book]`\n",
        "\n",
        "While modern deep learning frameworks can automate nearly all of this work, implementing things from scratch is the only way\n",
        "to make sure that you really know what you are doing.\n",
        "Moreover, when it comes time to customize models,\n",
        "defining our own layers or loss functions, understanding how things work under the hood will prove handy.\n",
        "\n",
        "In this section, we will rely only on **(i) tensors for data storage and linear algebra,\n",
        "and (ii) auto differentiation for calculating gradients**. Afterwards, in the second part, we will introduce a more concise implementation, taking advantage of bells and whistles of deep learning frameworks.\n"
      ],
      "metadata": {
        "id": "TRHAFheqABFi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "U6ZR8RG3a0U7"
      },
      "source": [
        "### Reading the dataset\n",
        "\n",
        "Recall that training models consists of\n",
        "making multiple passes over the dataset,\n",
        "grabbing one minibatch of examples at a time,\n",
        "and using them to update our model.\n",
        "Since this process is so fundamental\n",
        "to training machine learning algorithms,\n",
        "it is worth defining a **utility function\n",
        "to shuffle the dataset and access it in minibatches**.\n",
        "\n",
        "In the following code, we **define the `data_iter` function** to demonstrate one possible implementation of this functionality. The function **takes a batch size, a matrix of features,\n",
        "and a vector of labels, yielding minibatches of the size `batch_size`.** Each minibatch consists of a tuple of features and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 14,
        "tab": [
          "tensorflow"
        ],
        "id": "MY3PmFH_a0U8"
      },
      "outputs": [],
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # the examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        j = tf.constant(indices[i:min(i + batch_size, num_examples)])\n",
        "        yield tf.gather(features, j), tf.gather(labels, j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 15,
        "id": "WVrN37Gua0U9"
      },
      "source": [
        "In general, note that we want to use reasonably sized minibatches\n",
        "to take advantage of the GPU hardware,\n",
        "which excels at parallelizing operations.\n",
        "Because each example can be fed through our models in parallel\n",
        "and the gradient of the loss function for each example can also be taken in parallel,\n",
        "GPUs allow us to process hundreds of examples in scarcely more time\n",
        "than it might take to process just a single example.\n",
        "\n",
        "To build some intuition, let us read and print\n",
        "the first small batch of data examples.\n",
        "The shape of the features in each minibatch tells us\n",
        "both the minibatch size and the number of input features.\n",
        "Likewise, our minibatch of labels will have a shape given by `batch_size`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 16,
        "tab": [
          "tensorflow"
        ],
        "id": "a7qKYdmna0U9"
      },
      "outputs": [],
      "source": [
        "# minibatch size\n",
        "batch_size = 10\n",
        "\n",
        "for X, y in data_iter(batch_size, features, labels):\n",
        "    print(X, '\\n', y)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 17,
        "id": "DwUD-4Eja0U-"
      },
      "source": [
        "As we run the iteration, we obtain distinct minibatches\n",
        "successively until the entire dataset has been exhausted (try this).\n",
        "While the iteration implemented above is good for didactic purposes,\n",
        "it is inefficient in ways that might get us in trouble on real problems.\n",
        "For example, it requires that we load all the data in memory\n",
        "and that we perform lots of random memory access.\n",
        "The built-in iterators implemented in a deep learning framework\n",
        "are considerably more efficient and they can deal\n",
        "with both data stored in files and data fed via data streams.\n",
        "\n",
        "---\n",
        "\n",
        "### Initializing model parameters\n",
        "\n",
        "**Before we can begin optimizing our model's parameters** by minibatch stochastic gradient descent,\n",
        "**we need to have some parameters in the first place.**\n",
        "\n",
        "In the following code, we initialize weights by sampling\n",
        "random numbers from a normal distribution with mean 0\n",
        "and a standard deviation of 0.01, and setting the bias to 0. `trainable=True` means all weights and the bias will be updated by the optimizer during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 20,
        "tab": [
          "tensorflow"
        ],
        "id": "uIfnKHBUa0U_"
      },
      "outputs": [],
      "source": [
        "w = tf.Variable(tf.random.normal(shape=(2, 1), mean=0, stddev=0.01), trainable=True)\n",
        "b = tf.Variable(tf.zeros(1), trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 21,
        "id": "P4IF3Z0Wa0U_"
      },
      "source": [
        "After initializing our parameters,\n",
        "our next task is to update them until\n",
        "they fit our data sufficiently well.\n",
        "Each update requires taking the gradient\n",
        "of our loss function with respect to the parameters.\n",
        "Given this gradient, we can update each parameter\n",
        "in the direction that may reduce the loss.\n",
        "\n",
        "Since nobody wants to compute gradients explicitly (this is tedious and error prone), we use [automatic differentiation](https://www.tensorflow.org/guide/autodiff).\n",
        "\n",
        "---\n",
        "\n",
        "### Defining the model\n",
        "\n",
        "Next, we must **define our model,\n",
        "relating its inputs and parameters to its outputs.**\n",
        "\n",
        "Recall that to calculate the output of the linear model,\n",
        "we simply take the matrix-vector dot product\n",
        "of the input features $\\mathbf{X}$ and the model weights $\\mathbf{w}$,\n",
        "and add the offset $b$ to each example.\n",
        "\n",
        "Note that below $\\mathbf{Xw}$  is a vector and $b$ is a scalar, and recall that when we add a vector and a scalar,\n",
        "the scalar is added to each component of the vector (through the [broadcasting](https://www.tensorflow.org/xla/broadcasting) mechanism).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 22,
        "tab": [
          "tensorflow"
        ],
        "id": "MYx3yoCVa0VA"
      },
      "outputs": [],
      "source": [
        "def linreg(X, w, b):  \n",
        "    \"\"\"The linear regression model.\"\"\"\n",
        "    return tf.matmul(X, w) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 23,
        "id": "1HkBaTlRa0VA"
      },
      "source": [
        "## Defining the loss function\n",
        "\n",
        "Since **updating our model requires taking\n",
        "the gradient of our loss function**,\n",
        "we ought to **define the loss function first**.\n",
        "\n",
        "Here we will use the *squared loss function*.\n",
        "In the implementation, we need to transform the true value `y`\n",
        "into the predicted value's shape `y_hat`.\n",
        "The result returned by the following function\n",
        "will also have the same shape as `y_hat`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 24,
        "tab": [
          "tensorflow"
        ],
        "id": "P3gt2GrMa0VB"
      },
      "outputs": [],
      "source": [
        "def squared_loss(y_hat, y):  \n",
        "    \"\"\"Squared loss.\"\"\"\n",
        "    return (y_hat - tf.reshape(y, y_hat.shape))**2 / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 25,
        "id": "0fTccR8la0VB"
      },
      "source": [
        "## Defining the optimization algorithm\n",
        "\n",
        "Despite linear regression has a closed-form (analytic) solution, other deep learning models don't. So, we will use the **minibatch stochastic gradient descent** algorithm.\n",
        "\n",
        "At each step, using one minibatch randomly drawn from our dataset,\n",
        "we will estimate the gradient of the loss with respect to our parameters.\n",
        "Next, we will update our parameters\n",
        "in the direction that may reduce the loss.\n",
        "\n",
        "The following code applies the minibatch stochastic gradient descent update,\n",
        "given a `set of parameters`, a `learning rate`, and a `batch size`.\n",
        "The size of the update step is determined by the learning rate `lr`.\n",
        "Because our loss is calculated as a sum over the minibatch of examples,\n",
        "we normalize our step size by the batch size (`batch_size`),\n",
        "so that the magnitude of a typical step size\n",
        "does not depend heavily on our choice of the batch size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 28,
        "tab": [
          "tensorflow"
        ],
        "id": "_VCjnyGja0VB"
      },
      "outputs": [],
      "source": [
        "def sgd(params, grads, lr, batch_size):  \n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    for param, grad in zip(params, grads):\n",
        "        param.assign_sub(lr * grad / batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 29,
        "id": "cNLLnfh5a0VC"
      },
      "source": [
        "## training\n",
        "\n",
        "Now that we have all of the parts in place,\n",
        "we are ready to **implement the main training loop.**\n",
        "\n",
        "It is crucial that you understand this code\n",
        "because you will see nearly identical training loops\n",
        "over and over again throughout most deep learning courses and materials.\n",
        "\n",
        "In each iteration, we will grab a minibatch of training examples,\n",
        "and pass them through our model to obtain a set of predictions.\n",
        "After calculating the loss, we initiate the backwards pass through the network,\n",
        "storing the gradients with respect to each parameter.\n",
        "Finally, we will call the optimization algorithm `sgd`\n",
        "to update the model parameters.\n",
        "\n",
        "In summary, we will execute the following loop:\n",
        "\n",
        "* Initialize parameters $(\\mathbf{w}, b)$\n",
        "* Repeat until done\n",
        "    * Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n",
        "    * Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
        "\n",
        "In each *epoch*,\n",
        "we will iterate through the entire dataset\n",
        "(using the `data_iter` function) once\n",
        "passing through every example in the training dataset\n",
        "(assuming that the number of examples is divisible by the batch size).\n",
        "\n",
        "The number of epochs `num_epochs` and the learning rate `lr` are both hyperparameters,\n",
        "which we set here to 3 and 0.03, respectively.\n",
        "Unfortunately, setting hyperparameters is tricky\n",
        "and requires some adjustment by trial and error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 30,
        "tab": [
          "tensorflow"
        ],
        "id": "r9p41rQoa0VC"
      },
      "outputs": [],
      "source": [
        "lr = 0.03      # learning reate\n",
        "num_epochs = 3 # number of training steps\n",
        "net = linreg   # model\n",
        "loss = squared_loss # loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 33,
        "tab": [
          "tensorflow"
        ],
        "id": "k2Ug1_6na0VD"
      },
      "outputs": [],
      "source": [
        "# main training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter(batch_size, features, labels):\n",
        "        with tf.GradientTape() as g:\n",
        "          # Minibatch loss in X and y\n",
        "          l = loss(net(X, w, b), y)  \n",
        "          # compute gradient on l with respect to [w, b]\n",
        "          dw, db = g.gradient(l, [w, b])\n",
        "          # update parameters using their gradient\n",
        "          sgd([w, b], [dw, db], lr, batch_size)\n",
        "    train_l = loss(net(features, w, b), labels)\n",
        "    print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)):f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 34,
        "id": "wGV9jzsha0VD"
      },
      "source": [
        "In this case, because we synthesized the dataset ourselves,\n",
        "we know precisely what the true parameters are.\n",
        "Thus, we can **evaluate our success in training\n",
        "by comparing the true parameters\n",
        "with those that we learned** through our training loop.\n",
        "Indeed they turn out to be very close to each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 35,
        "tab": [
          "tensorflow"
        ],
        "id": "CyYR30Zba0VD"
      },
      "outputs": [],
      "source": [
        "print(f'Error in estimating w: {true_w - tf.reshape(w, true_w.shape)}')\n",
        "print(f'Error in estimating b: {true_b - b}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 36,
        "id": "XDafuMUMa0VE"
      },
      "source": [
        "Note that we should not take it for granted\n",
        "that we are able to recover the parameters perfectly.\n",
        "However, in machine learning, we are typically less concerned\n",
        "with recovering true underlying parameters,\n",
        "and more concerned with parameters that lead to highly accurate prediction.\n",
        "Fortunately, even on difficult optimization problems,\n",
        "stochastic gradient descent can often find remarkably good solutions,\n",
        "owing partly to the fact that, for deep networks,\n",
        "there exist many configurations of the parameters\n",
        "that lead to highly accurate prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of the first approach\n",
        "\n",
        "* We saw how a deep network can be implemented and optimized from scratch, using just tensors and auto differentiation, without any need for defining layers or fancy optimizers.\n",
        "* This section only scratches the surface of what is possible. In the following sections, we will describe additional models based on the concepts that we have just introduced and learn how to implement them more concisely.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Second approach: linear regression model using high-level APIs\n",
        "\n",
        "`[from the D2L reference book]`\n",
        "\n",
        "Broad and intense interest in deep learning for the past several years\n",
        "has inspired companies, academics, and hobbyists\n",
        "to develop a variety of mature open source frameworks\n",
        "for automating the repetitive work of implementing\n",
        "gradient-based learning algorithms.\n",
        "\n",
        "In the first approach, we relied only on\n",
        "tensors and auto differentiation for calculating gradients. In practice, because data iterators, loss functions, optimizers, and neural network layers are so common, modern libraries implement these components for us as well.\n",
        "\n",
        "In this section, we will show you how to implement\n",
        "the linear regression model concisely by using **high-level APIs from TensorFlow**."
      ],
      "metadata": {
        "id": "Ac4pCZy978S8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the dataset\n",
        "\n",
        "Rather than rolling our own iterator,\n",
        "we can call upon the existing [`data` API](https://www.tensorflow.org/guide/data) to read data.\n",
        "\n",
        "We pass in `features` and `labels` as arguments and specify `batch_size`\n",
        "when instantiating a data iterator object.\n",
        "Besides, the boolean value `is_train`\n",
        "indicates whether or not\n",
        "we want the data iterator object to shuffle the data\n",
        "on each epoch (pass through the dataset)."
      ],
      "metadata": {
        "id": "ASXe6qayBEqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset through the 'data' high-level API\n",
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    \"\"\"Construct a TensorFlow data iterator.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(buffer_size=1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "rfPZkeVt8AR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use `data_iter` in much the same way as we called the `data_iter` function in the first part.\n",
        "\n",
        "To verify that it is working, we can read and print\n",
        "the first minibatch of examples. Comparing with our previous approach, here we use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator."
      ],
      "metadata": {
        "id": "h_xIq9gzBVBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "data_iter = load_array((features, labels), batch_size)"
      ],
      "metadata": {
        "id": "AWyf5qnz8AVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(data_iter))"
      ],
      "metadata": {
        "id": "AQBegOVi8AY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the model\n",
        "\n",
        "When we implemented linear regression from scratch,\n",
        "we defined our model parameters explicitly\n",
        "and coded up the calculations to produce output\n",
        "using basic linear algebra operations. But once your models get more complex,\n",
        "and once you have to do this nearly every day,\n",
        "you will be glad for the assistance.\n",
        "\n",
        "For standard operations, we can use predefined layers from the [Keras Sequential model](https://keras.io/guides/sequential_model/),\n",
        "which allow us to focus especially\n",
        "on the layers used to construct the model\n",
        "rather than having to focus on the implementation.\n",
        "\n",
        "We will first define a model variable `net`,\n",
        "which will refer to an instance of the `Sequential` class, which defines a container\n",
        "for several layers that will be chained together.\n",
        "Given input data, a `Sequential` instance passes it through\n",
        "the first layer, in turn passing the output\n",
        "as the second layer's input and so forth.\n",
        "\n",
        "In the following example, our model consists of only one layer,\n",
        "so we do not really need `Sequential`.\n",
        "But since nearly all of our future models\n",
        "will involve multiple layers,\n",
        "we will use it anyway just to familiarize you\n",
        "with the most standard workflow.\n",
        "\n",
        "Recall the architecture of a single-layer network.\n",
        "The layer is said to be *fully-connected*\n",
        "because each of its inputs is connected to each of its outputs by means of a matrix-vector multiplication."
      ],
      "metadata": {
        "id": "rbcsnOrZBs6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see this figure, download it from the GitHub repo 'fig' folder, upload it into Colab, and then uncomment the following lines.\n",
        "#from IPython.display import Image\n",
        "#Image(filename='./w01_singleneuron.png', width='500') "
      ],
      "metadata": {
        "id": "BvDr5SayDPVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the `Keras Sequential model`, the fully-connected layer is defined in the `Dense` class. Since we only want to generate a single scalar output, we set that number to 1.\n",
        "\n",
        "It is worth noting that, for convenience,\n",
        "Keras does not require us to specify\n",
        "the input shape for each layer.\n",
        "So here, we do not need to tell Keras\n",
        "how many inputs go into this linear layer.\n",
        "When we first try to pass data through our model,\n",
        "e.g., when we execute `net(X)` later,\n",
        "Keras will automatically infer the number of inputs to each layer.\n",
        "We will describe how this works in more detail later.\n"
      ],
      "metadata": {
        "id": "MnzHPuN3DPqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `keras` is the high-level API for TensorFlow\n",
        "net = tf.keras.Sequential()\n",
        "net.add(tf.keras.layers.Dense(1))"
      ],
      "metadata": {
        "id": "inU5E9Dh8AbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing model parameters\n",
        "\n",
        "Before using `net`, we need to **initialize the model parameters**,\n",
        "such as the weights and bias in the linear regression model.\n",
        "\n",
        "Deep learning frameworks have a predefined way to initialize the parameters. Here we specify that each weight parameter\n",
        "should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01. The bias parameter will be initialized to zero.\n",
        "\n",
        "The [initializers module in TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/initializers) provides various methods for model parameter initialization. The easiest way to specify the initialization method in Keras is when creating the layer by specifying `kernel_initializer`. Here we recreate `net`, adding the initializer."
      ],
      "metadata": {
        "id": "Ce-uosg2ESB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initializer = tf.initializers.RandomNormal(stddev=0.01)\n",
        "# model definition\n",
        "net = tf.keras.Sequential()\n",
        "# model's first layer\n",
        "net.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))"
      ],
      "metadata": {
        "id": "uxplLQ888AeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is an important aspect regarding the above code: we are initializing parameters for a network\n",
        "even though Keras does not yet know\n",
        "how many dimensions the input will have!\n",
        "It might be 2 as in our example or it might be 2000.\n",
        "\n",
        "Keras lets us get away with this because, behind the scenes,\n",
        "the initialization is actually *deferred*.\n",
        "The real initialization will take place only\n",
        "when we, for the first time, attempt to pass data through the network.\n",
        "Just be careful to remember that since the parameters\n",
        "have not been initialized yet,\n",
        "we cannot access or manipulate them."
      ],
      "metadata": {
        "id": "Nec8b7PnEYvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the loss function\n",
        "\n",
        "The `MeanSquaredError` class computes the mean squared error, also known as squared $L_2$ norm. By default it returns the average loss over examples.\n",
        "\n",
        "Check other options in the [tf.keras.losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses) module."
      ],
      "metadata": {
        "id": "26gj6SfxEpmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.MeanSquaredError()"
      ],
      "metadata": {
        "id": "VVV_etqx8Age"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the optimization algorithm\n",
        "\n",
        "Minibatch stochastic gradient descent is a standard tool\n",
        "for optimizing neural networks\n",
        "and thus Keras supports it alongside a number of\n",
        "variations on this algorithm in the [optimizers module](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
        "Minibatch stochastic gradient descent just requires that\n",
        "we set the value `learning_rate`, which is set to 0.03 here."
      ],
      "metadata": {
        "id": "OjzbMCkiExVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = tf.keras.optimizers.SGD(learning_rate=0.03)"
      ],
      "metadata": {
        "id": "BQUFgSGV8wOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "\n",
        "You might have noticed that expressing our model through\n",
        "high-level APIs of a deep learning framework\n",
        "requires comparatively few lines of code.\n",
        "We did not have to individually allocate parameters,\n",
        "define our loss function, or implement minibatch stochastic gradient descent.\n",
        "Once we start working with much more complex models,\n",
        "advantages of high-level APIs will grow considerably.\n",
        "\n",
        "However, once we have all the basic pieces in place,\n",
        "**the training loop itself is strikingly similar\n",
        "to what we did when implementing everything from scratch**: for some number of epochs,\n",
        "we will make a complete pass over the dataset (`train_data`),\n",
        "iteratively grabbing one minibatch of inputs\n",
        "and the corresponding ground-truth labels.\n",
        "For each minibatch, we go through the following steps:\n",
        "\n",
        "* Generate predictions by calling `net(X)` and calculate the loss `l` (the forward propagation).\n",
        "* Calculate gradients by running the backpropagation.\n",
        "* Update the model parameters by invoking our optimizer.\n",
        "\n",
        "For good measure, we compute the loss after each epoch and print it to monitor progress.\n"
      ],
      "metadata": {
        "id": "ZBMo3tAwE43y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of training steps\n",
        "num_epochs = 3\n",
        "# iterate over training steps\n",
        "for epoch in range(num_epochs):\n",
        "  for X, y in data_iter:\n",
        "    with tf.GradientTape() as tape:\n",
        "        # generating predictions and calculate loss\n",
        "        l = loss(net(X, training=True), y)\n",
        "    # calculate gradients (backpropagation)\n",
        "    grads = tape.gradient(l, net.trainable_variables)\n",
        "    # update model parameters\n",
        "    trainer.apply_gradients(zip(grads, net.trainable_variables))\n",
        "  l = loss(net(features), labels)\n",
        "  print(f'epoch {epoch + 1}, loss {l:f}')"
      ],
      "metadata": {
        "id": "SOirmrQI8wRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we **compare the model parameters learned by training on finite data\n",
        "and the actual parameters** that generated our dataset.\n",
        "To access parameters,\n",
        "we first access the layer that we need from `net`\n",
        "and then access that layer's weights and bias.\n",
        "As in our from-scratch implementation,\n",
        "note that our estimated parameters are\n",
        "close to their ground-truth counterparts.\n"
      ],
      "metadata": {
        "id": "oX811XZyFEyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = net.get_weights()[0]\n",
        "print('Error in estimating w:', true_w - tf.reshape(w, true_w.shape))\n",
        "b = net.get_weights()[1]\n",
        "print('Error in estimating b:', true_b - b)"
      ],
      "metadata": {
        "id": "IIpEFokx8wUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of the second approach\n",
        "\n",
        "* Using TensorFlow's high-level APIs, we can implement models much more concisely.\n",
        "* In TensorFlow, the `data` module provides tools for data processing, the `keras` module defines a large number of neural network layers and common loss functions.\n",
        "* TensorFlow's module `initializers` provides various methods for model parameter initialization.\n",
        "* Dimensionality and storage are automatically inferred (but be careful not to attempt to access parameters before they have been initialized).\n"
      ],
      "metadata": {
        "id": "YvTXvQm68_Cq"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "w01_linear-regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}